使用方法
基本用法：

```bash
python web_crawler.py
```
使用配置文件：

```bash
python web_crawler.py --config crawler_config.yml
```
指定输出目录和格式：

```bash
python web_crawler.py --output data/medical --format json
```
调整线程数：

```bash
python web_crawler.py --workers 10
```
启用调试模式：

```bash
python web_crawler.py --debug
```

特点和功能
多站点抓取：支持同时抓取多个医疗网站的数据

消化内科专用：内置消化内科关键词过滤，确保爬取的数据与消化内科相关

反爬虫措施：

随机UA
随机延迟
代理IP支持
尊重robots.txt
数据处理和过滤：

提取文章标题、内容、作者、发布时间
过滤内容长度不足的文章
提取相关疾病名称

错误处理：
自动重试机制
连接错误处理
异常捕获和记录


可配置性：
    - YAML配置文件支持
    - 命令行参数覆盖
    - 灵活的数据存储格式
    - 多线程并发：使用线程池提高爬取效率

状态保存：记录已爬取的URL，支持**断点续爬**

质量控制：只保存与消化内科相关、内容足够丰富的文章