#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åˆå¹¶SFT LoRAæ¨¡å‹
å°†åŸºç¡€æ¨¡å‹ + SFT LoRA åˆå¹¶ä¸ºå®Œæ•´çš„SFTæ¨¡å‹
"""

import os
import sys
import torch
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

def merge_sft_model(
    base_model_path: str = "Qwen2.5-1.5B-Instruct/qwen/Qwen2___5-1___5B-Instruct",
    sft_lora_path: str = "model_output/qwen2_5_1_5b_instruct_sft",
    output_path: str = "model_output/qwen2_5_1_5b_instruct_sft_merged"
):
    """
    åˆå¹¶SFT LoRAåˆ°åŸºç¡€æ¨¡å‹
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        sft_lora_path: SFT LoRAé€‚é…å™¨è·¯å¾„
        output_path: è¾“å‡ºå®Œæ•´æ¨¡å‹è·¯å¾„
    """
    
    print("=" * 70)
    print("SFTæ¨¡å‹åˆå¹¶å·¥å…·")
    print("=" * 70)
    
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    if not os.path.isabs(base_model_path):
        base_model_path = os.path.join(project_root, base_model_path)
    if not os.path.isabs(sft_lora_path):
        sft_lora_path = os.path.join(project_root, sft_lora_path)
    if not os.path.isabs(output_path):
        output_path = os.path.join(project_root, output_path)
    
    print(f"\nğŸ“‚ è·¯å¾„é…ç½®:")
    print(f"  åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"  SFT LoRA: {sft_lora_path}")
    print(f"  è¾“å‡ºè·¯å¾„: {output_path}")
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(base_model_path):
        print(f"\nâŒ é”™è¯¯: åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}")
        return None
    
    if not os.path.exists(sft_lora_path):
        print(f"\nâŒ é”™è¯¯: SFT LoRAè·¯å¾„ä¸å­˜åœ¨: {sft_lora_path}")
        return None
    
    # æ­¥éª¤1: åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"\næ­¥éª¤1: åŠ è½½åŸºç¡€æ¨¡å‹...")
    print(f"  (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...)")
    
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True
    )
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    print("âœ“ åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # æ­¥éª¤2: åŠ è½½SFT LoRA
    print(f"\næ­¥éª¤2: åŠ è½½SFT LoRAé€‚é…å™¨...")
    model_with_lora = PeftModel.from_pretrained(base_model, sft_lora_path)
    print("âœ“ SFT LoRAåŠ è½½å®Œæˆ")
    
    # æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in model_with_lora.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model_with_lora.parameters())
    print(f"\nğŸ“Š æ¨¡å‹å‚æ•°:")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  LoRAå‚æ•°å æ¯”: {100 * trainable_params / total_params:.2f}%")
    
    # æ­¥éª¤3: åˆå¹¶LoRA
    print(f"\næ­¥éª¤3: åˆå¹¶SFT LoRAåˆ°åŸºç¡€æ¨¡å‹...")
    print(f"  (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...)")
    merged_model = model_with_lora.merge_and_unload()
    print("âœ“ LoRAå·²åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹")
    
    # æ­¥éª¤4: ä¿å­˜å®Œæ•´æ¨¡å‹
    print(f"\næ­¥éª¤4: ä¿å­˜å®Œæ•´æ¨¡å‹...")
    os.makedirs(output_path, exist_ok=True)
    
    print(f"  ä¿å­˜æ¨¡å‹æƒé‡...")
    merged_model.save_pretrained(
        output_path,
        safe_serialization=True  # ä½¿ç”¨safetensorsæ ¼å¼
    )
    
    print(f"  ä¿å­˜tokenizer...")
    tokenizer.save_pretrained(output_path)
    
    print("âœ“ æ¨¡å‹å·²ä¿å­˜")
    
    # æ­¥éª¤5: éªŒè¯ä¿å­˜çš„æ–‡ä»¶
    print(f"\næ­¥éª¤5: éªŒè¯ä¿å­˜çš„æ–‡ä»¶...")
    saved_files = []
    total_size = 0
    
    for file in os.listdir(output_path):
        file_path = os.path.join(output_path, file)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path)
            total_size += size
            size_mb = size / (1024 * 1024)
            saved_files.append((file, size_mb))
    
    print(f"\nğŸ“Š ä¿å­˜çš„æ–‡ä»¶:")
    for file, size_mb in sorted(saved_files, key=lambda x: -x[1]):
        if size_mb > 1:  # åªæ˜¾ç¤ºå¤§äº1MBçš„æ–‡ä»¶
            print(f"  {file}: {size_mb:.1f} MB")
    
    total_size_gb = total_size / (1024 * 1024 * 1024)
    print(f"\n  æ€»å¤§å°: {total_size_gb:.2f} GB")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´æ¨¡å‹ï¼ˆä¸æ˜¯LoRAï¼‰
    has_adapter_config = os.path.exists(os.path.join(output_path, "adapter_config.json"))
    has_model_weights = any(
        f.endswith(('.safetensors', '.bin')) and 'adapter' not in f
        for f in os.listdir(output_path)
    )
    
    if has_adapter_config:
        print("\nâš ï¸  è­¦å‘Š: å‘ç°adapter_config.jsonï¼Œè¿™å¯èƒ½ä»ç„¶æ˜¯LoRAé€‚é…å™¨")
    
    if not has_model_weights:
        print("\nâš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°å®Œæ•´æ¨¡å‹æƒé‡æ–‡ä»¶")
    
    if has_model_weights and not has_adapter_config:
        print("\nâœ… éªŒè¯é€šè¿‡: è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„åˆå¹¶æ¨¡å‹")
    
    # å®Œæˆ
    print("\n" + "=" * 70)
    print("âœ… SFTæ¨¡å‹åˆå¹¶å®Œæˆï¼")
    print("=" * 70)
    print(f"\nå®Œæ•´SFTæ¨¡å‹ä¿å­˜åœ¨: {output_path}")
    print("\nç°åœ¨ä½ å¯ä»¥:")
    print("1. è¯„ä¼°SFTæ¨¡å‹:")
    print(f"   model_path: {output_path}")
    print(f"   is_lora: false")
    print("\n2. åœ¨æ­¤åŸºç¡€ä¸Šè®­ç»ƒDPO:")
    print(f"   base_model_path: {output_path}")
    print(f"   is_lora: false")
    
    return output_path

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆå¹¶SFT LoRAæ¨¡å‹")
    parser.add_argument(
        "--base_model",
        default="Qwen2.5-1.5B-Instruct/qwen/Qwen2___5-1___5B-Instruct",
        help="åŸºç¡€æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--sft_lora",
        default="model_output/qwen2_5_1_5b_instruct_sft",
        help="SFT LoRAè·¯å¾„"
    )
    parser.add_argument(
        "--output",
        default="model_output/qwen2_5_1_5b_instruct_sft_merged",
        help="è¾“å‡ºè·¯å¾„"
    )
    
    args = parser.parse_args()
    
    try:
        output_path = merge_sft_model(
            base_model_path=args.base_model,
            sft_lora_path=args.sft_lora,
            output_path=args.output
        )
        
        if output_path:
            print(f"\nâœ… æˆåŠŸï¼")
            sys.exit(0)
        else:
            print(f"\nâŒ å¤±è´¥")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
# python scripts/merge_sft_model.py 