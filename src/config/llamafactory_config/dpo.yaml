# configs/dpo.yaml
model_name_or_path: qwen2-7b
dataset_dir: data/processed/dpo
output_dir: models/qwen2-7b-medical

training_type: dpo
dpo_beta: 0.1
dpo_loss_type: sigmoid

per_device_train_batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 1e-5
max_length: 2048

flash_attn: true
rope_scaling: linear
load_in_4bit: true