#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åŒ»ç–—QAæ¨¡å‹æ¨ç†å·¥å…·
æ”¯æŒSFTå’ŒDPOæ¨¡å‹çš„æ¨ç†ï¼Œæ”¯æŒLoRAæ¨¡å‹åŠ è½½å’Œåˆå¹¶
"""

import os
import sys
import json
import yaml
import torch
from pathlib import Path
from typing import List, Dict, Optional, Union

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = Path(__file__).resolve().parent
src_dir = script_dir.parent
project_root = src_dir.parent
sys.path.insert(0, str(project_root))

from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from peft import PeftModel


def load_system_prompt(config_path: str = "config/system_prompt.yaml") -> str:
    """
    åŠ è½½ç³»ç»Ÿæç¤º
    
    Args:
        config_path: ç³»ç»Ÿæç¤ºé…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        ç³»ç»Ÿæç¤ºå­—ç¬¦ä¸²
    """
    try:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            config_path,
            os.path.join(project_root, config_path),
            os.path.join(os.getcwd(), config_path),
        ]
        
        for full_path in possible_paths:
            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    if config and 'system_prompt' in config:
                        return config['system_prompt']
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•åŠ è½½ç³»ç»Ÿæç¤ºé…ç½® {config_path}: {e}")
    
    # é»˜è®¤ç³»ç»Ÿæç¤º
    return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—å¥åº·ä¿¡æ¯åŠ©æ‰‹ï¼Œå…·å¤‡å…¨ç§‘åŒ»å­¦åŸºç¡€çŸ¥è¯†ã€‚
è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ä½¿ç”¨ä¸ç¡®å®šæ€§è¡¨è¿°ï¼ˆ"å¯èƒ½æ˜¯"ã€"è€ƒè™‘"ã€"å¸¸è§åŸå› åŒ…æ‹¬"ï¼‰
2. å»ºè®®æ£€æŸ¥é¡¹ç›®å’Œå°±åŒ»ç§‘å®¤ï¼Œä½†ä¸åšæ˜ç¡®è¯Šæ–­
3. ä¸¥é‡ç—‡çŠ¶å¿…é¡»å»ºè®®å°±åŒ»
4. ä¸ç¼–é€ ä¿¡æ¯ï¼Œä¸ç¡®å®šæ—¶å¼•å¯¼ä¸“ä¸šå°±åŒ»"""


class MedicalQAInference:
    """åŒ»ç–—QAæ¨ç†ç±»"""
    
    def __init__(
        self,
        model_path: str,
        base_model_path: Optional[str] = None,
        is_lora: bool = False,
        merge_lora: bool = True,
        system_prompt: Optional[str] = None,
        device: str = "cuda",
        load_in_8bit: bool = False,
        load_in_4bit: bool = False,
    ):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å®Œæ•´æ¨¡å‹æˆ–LoRAé€‚é…å™¨ï¼‰
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆä»…å½“is_lora=Trueæ—¶éœ€è¦ï¼‰
            is_lora: æ˜¯å¦æ˜¯LoRAæ¨¡å‹
            merge_lora: æ˜¯å¦åˆå¹¶LoRAæƒé‡ï¼ˆæ¨èTrueä»¥åŠ é€Ÿæ¨ç†ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœä¸ºNoneï¼Œä»é…ç½®æ–‡ä»¶åŠ è½½ï¼‰
            device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
            load_in_8bit: æ˜¯å¦ä»¥8bitåŠ è½½ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
            load_in_4bit: æ˜¯å¦ä»¥4bitåŠ è½½ï¼ˆæ›´çœæ˜¾å­˜ï¼‰
        """
        self.device = device if torch.cuda.is_available() else "cpu"
        self.model_path = model_path
        self.is_lora = is_lora
        
        # åŠ è½½ç³»ç»Ÿæç¤º
        if system_prompt is None:
            self.system_prompt = load_system_prompt()
            print("âœ“ ä»é…ç½®æ–‡ä»¶åŠ è½½ç³»ç»Ÿæç¤º")
        else:
            self.system_prompt = system_prompt
            print("âœ“ ä½¿ç”¨ä¼ å…¥çš„ç³»ç»Ÿæç¤º")
        
        print(f"\n{'='*60}")
        print("åˆå§‹åŒ–åŒ»ç–—QAæ¨ç†å™¨")
        print(f"{'='*60}")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"æ˜¯å¦LoRA: {is_lora}")
        if is_lora:
            print(f"åŸºç¡€æ¨¡å‹: {base_model_path}")
            print(f"åˆå¹¶LoRA: {merge_lora}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"{'='*60}\n")
        
        # åŠ è½½tokenizer
        print("ğŸ“¥ åŠ è½½Tokenizer...")
        tokenizer_path = base_model_path if is_lora else model_path
        self.tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            trust_remote_code=True,
            padding_side='left'
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("âœ“ TokenizeråŠ è½½å®Œæˆ")
        
        # åŠ è½½æ¨¡å‹
        print("\nğŸ“¥ åŠ è½½æ¨¡å‹...")
        
        # é…ç½®é‡åŒ–å‚æ•°
        load_kwargs = {
            'trust_remote_code': True,
            'device_map': 'auto' if self.device == 'cuda' else None,
        }
        
        if not (load_in_8bit or load_in_4bit):
            load_kwargs['torch_dtype'] = torch.bfloat16 if torch.cuda.is_available() else torch.float32
        
        if load_in_8bit:
            load_kwargs['load_in_8bit'] = True
            print("  ä½¿ç”¨8bité‡åŒ–åŠ è½½")
        elif load_in_4bit:
            load_kwargs['load_in_4bit'] = True
            print("  ä½¿ç”¨4bité‡åŒ–åŠ è½½")
        
        if is_lora:
            # åŠ è½½åŸºç¡€æ¨¡å‹ + LoRAé€‚é…å™¨
            print(f"  åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                **load_kwargs
            )
            
            print(f"  åŠ è½½LoRAé€‚é…å™¨: {model_path}")
            model = PeftModel.from_pretrained(base_model, model_path)
            
            if merge_lora:
                print("  åˆå¹¶LoRAæƒé‡...")
                model = model.merge_and_unload()
                print("  âœ“ LoRAæƒé‡å·²åˆå¹¶")
        else:
            # åŠ è½½å®Œæ•´æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **load_kwargs
            )
        
        model.eval()
        self.model = model
        
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    def generate(
        self,
        question: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        repetition_penalty: float = 1.0,
        do_sample: bool = True,
    ) -> str:
        """
        ç”Ÿæˆå•ä¸ªé—®é¢˜çš„å›ç­”
        
        Args:
            question: é—®é¢˜
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
            top_p: nucleus sampling
            top_k: top-k sampling
            repetition_penalty: é‡å¤æƒ©ç½š
            do_sample: æ˜¯å¦é‡‡æ ·
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        # æ„å»ºè¾“å…¥
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": question}
        ]
        
        # ä½¿ç”¨chat template
        if hasattr(self.tokenizer, "apply_chat_template"):
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # æ‰‹åŠ¨æ„å»ºï¼ˆQwenæ ¼å¼ï¼‰
            text = f"<|im_start|>system\n{self.system_prompt}<|im_end|>\n"
            text += f"<|im_start|>user\n{question}<|im_end|>\n"
            text += "<|im_start|>assistant\n"
        
        # Tokenize
        inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # è§£ç 
        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        return response.strip()
    
    def batch_generate(
        self,
        questions: List[str],
        batch_size: int = 4,
        **generate_kwargs
    ) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆå›ç­”
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            **generate_kwargs: ä¼ é€’ç»™generateçš„å…¶ä»–å‚æ•°
            
        Returns:
            å›ç­”åˆ—è¡¨
        """
        from tqdm import tqdm
        
        answers = []
        
        for i in tqdm(range(0, len(questions), batch_size), desc="æ‰¹é‡æ¨ç†"):
            batch = questions[i:i+batch_size]
            for question in batch:
                answer = self.generate(question, **generate_kwargs)
                answers.append(answer)
        
        return answers
    
    def interactive_chat(self):
        """äº¤äº’å¼å¯¹è¯æ¨¡å¼"""
        print("\n" + "="*60)
        print("åŒ»ç–—QAäº¤äº’å¼å¯¹è¯")
        print("="*60)
        print("è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("="*60 + "\n")
        
        while True:
            try:
                question = input("\nğŸ‘¤ ç”¨æˆ·: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("\nå†è§ï¼")
                    break
                
                if not question:
                    continue
                
                print("\nğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
                answer = self.generate(question)
                print(answer)
                
            except KeyboardInterrupt:
                print("\n\nå†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åŒ»ç–—QAæ¨¡å‹æ¨ç†")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--base_model_path",
        type=str,
        default=None,
        help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆä»…å½“ä½¿ç”¨LoRAæ—¶éœ€è¦ï¼‰"
    )
    parser.add_argument(
        "--is_lora",
        action="store_true",
        help="æ˜¯å¦æ˜¯LoRAæ¨¡å‹"
    )
    parser.add_argument(
        "--no_merge",
        action="store_true",
        help="ä¸åˆå¹¶LoRAï¼ˆé»˜è®¤åˆå¹¶ï¼‰"
    )
    parser.add_argument(
        "--question",
        type=str,
        default=None,
        help="å•ä¸ªé—®é¢˜ï¼ˆå¦‚æœä¸æä¾›ï¼Œè¿›å…¥äº¤äº’æ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--questions_file",
        type=str,
        default=None,
        help="é—®é¢˜æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆæ‰¹é‡æ¨ç†æ—¶ï¼‰"
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=512,
        help="æœ€å¤§ç”Ÿæˆtokenæ•°"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="æ¸©åº¦"
    )
    parser.add_argument(
        "--top_p",
        type=float,
        default=0.9,
        help="top_p"
    )
    parser.add_argument(
        "--load_in_8bit",
        action="store_true",
        help="ä»¥8bitåŠ è½½æ¨¡å‹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰"
    )
    parser.add_argument(
        "--load_in_4bit",
        action="store_true",
        help="ä»¥4bitåŠ è½½æ¨¡å‹ï¼ˆæ›´çœæ˜¾å­˜ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    inferencer = MedicalQAInference(
        model_path=args.model_path,
        base_model_path=args.base_model_path,
        is_lora=args.is_lora,
        merge_lora=not args.no_merge,
        load_in_8bit=args.load_in_8bit,
        load_in_4bit=args.load_in_4bit,
    )
    
    # å•ä¸ªé—®é¢˜
    if args.question:
        print(f"\né—®é¢˜: {args.question}\n")
        answer = inferencer.generate(
            args.question,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
        )
        print(f"å›ç­”: {answer}\n")
    
    # æ‰¹é‡é—®é¢˜
    elif args.questions_file:
        print(f"\nä»æ–‡ä»¶åŠ è½½é—®é¢˜: {args.questions_file}")
        
        with open(args.questions_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æ”¯æŒå¤šç§æ ¼å¼
        if isinstance(data, list):
            if isinstance(data[0], str):
                questions = data
            elif isinstance(data[0], dict):
                questions = [item.get('question') or item.get('query') or '' for item in data]
        else:
            raise ValueError("ä¸æ”¯æŒçš„é—®é¢˜æ–‡ä»¶æ ¼å¼")
        
        print(f"å…± {len(questions)} ä¸ªé—®é¢˜\n")
        
        # æ‰¹é‡ç”Ÿæˆ
        answers = inferencer.batch_generate(
            questions,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
        )
        
        # ä¿å­˜ç»“æœ
        results = [
            {"question": q, "answer": a}
            for q, a in zip(questions, answers)
        ]
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        else:
            # æ‰“å°å‰3ä¸ª
            print("\n" + "="*60)
            print("ç¤ºä¾‹ç»“æœï¼ˆå‰3ä¸ªï¼‰:")
            print("="*60)
            for i, result in enumerate(results[:3], 1):
                print(f"\n[{i}] é—®é¢˜: {result['question']}")
                print(f"    å›ç­”: {result['answer'][:200]}...")
    
    # äº¤äº’æ¨¡å¼
    else:
        inferencer.interactive_chat()


if __name__ == "__main__":
    main()
