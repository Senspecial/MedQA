#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åŒ»ç–—QAæ¨¡å‹æ¨ç†å·¥å…·
æ”¯æŒSFTå’ŒDPOæ¨¡å‹çš„æ¨ç†ï¼Œæ”¯æŒLoRAæ¨¡å‹åŠ è½½å’Œåˆå¹¶
æ”¯æŒ HuggingFace Transformers å’Œ vLLM ä¸¤ç§æ¨ç†åç«¯
"""

import os
import sys
import json
import yaml
import torch
from pathlib import Path
from typing import List, Dict, Optional, Union

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = Path(__file__).resolve().parent
src_dir = script_dir.parent
project_root = src_dir.parent
sys.path.insert(0, str(project_root))

from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from peft import PeftModel

# å°è¯•å¯¼å…¥ vLLM
try:
    from vllm import LLM, SamplingParams
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False


def load_system_prompt(config_path: str = "config/system_prompt.yaml") -> str:
    """
    åŠ è½½ç³»ç»Ÿæç¤º
    
    Args:
        config_path: ç³»ç»Ÿæç¤ºé…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        ç³»ç»Ÿæç¤ºå­—ç¬¦ä¸²
    """
    try:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
        possible_paths = [
            config_path,
            os.path.join(project_root, config_path),
            os.path.join(os.getcwd(), config_path),
        ]
        
        for full_path in possible_paths:
            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    if config and 'system_prompt' in config:
                        return config['system_prompt']
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•åŠ è½½ç³»ç»Ÿæç¤ºé…ç½® {config_path}: {e}")
    
    # é»˜è®¤ç³»ç»Ÿæç¤º
    return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—å¥åº·ä¿¡æ¯åŠ©æ‰‹ï¼Œå…·å¤‡å…¨ç§‘åŒ»å­¦åŸºç¡€çŸ¥è¯†ã€‚
è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ä½¿ç”¨ä¸ç¡®å®šæ€§è¡¨è¿°ï¼ˆ"å¯èƒ½æ˜¯"ã€"è€ƒè™‘"ã€"å¸¸è§åŸå› åŒ…æ‹¬"ï¼‰
2. å»ºè®®æ£€æŸ¥é¡¹ç›®å’Œå°±åŒ»ç§‘å®¤ï¼Œä½†ä¸åšæ˜ç¡®è¯Šæ–­
3. ä¸¥é‡ç—‡çŠ¶å¿…é¡»å»ºè®®å°±åŒ»
4. ä¸ç¼–é€ ä¿¡æ¯ï¼Œä¸ç¡®å®šæ—¶å¼•å¯¼ä¸“ä¸šå°±åŒ»"""


class MedicalQAInference:
    """åŒ»ç–—QAæ¨ç†ç±»ï¼Œæ”¯æŒ HuggingFace Transformers å’Œ vLLM ä¸¤ç§åç«¯"""
    
    def __init__(
        self,
        model_path: str,
        base_model_path: Optional[str] = None,
        is_lora: bool = False,
        merge_lora: bool = True,
        system_prompt: Optional[str] = None,
        device: str = "cuda",
        load_in_8bit: bool = False,
        load_in_4bit: bool = False,
        use_vllm: bool = False,
        vllm_gpu_memory_utilization: float = 0.9,
        vllm_tensor_parallel_size: int = 1,
        vllm_max_model_len: int = 4096,
        vllm_dtype: str = "auto",
    ):
        """
        åˆå§‹åŒ–æ¨ç†å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¯ä»¥æ˜¯å®Œæ•´æ¨¡å‹æˆ–LoRAé€‚é…å™¨ï¼‰
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆä»…å½“is_lora=Trueæ—¶éœ€è¦ï¼‰
            is_lora: æ˜¯å¦æ˜¯LoRAæ¨¡å‹
            merge_lora: æ˜¯å¦åˆå¹¶LoRAæƒé‡ï¼ˆæ¨èTrueä»¥åŠ é€Ÿæ¨ç†ï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœä¸ºNoneï¼Œä»é…ç½®æ–‡ä»¶åŠ è½½ï¼‰
            device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰ï¼Œä»…å¯¹HuggingFaceåç«¯æœ‰æ•ˆ
            load_in_8bit: æ˜¯å¦ä»¥8bitåŠ è½½ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰ï¼Œä»…å¯¹HuggingFaceåç«¯æœ‰æ•ˆ
            load_in_4bit: æ˜¯å¦ä»¥4bitåŠ è½½ï¼ˆæ›´çœæ˜¾å­˜ï¼‰ï¼Œä»…å¯¹HuggingFaceåç«¯æœ‰æ•ˆ
            use_vllm: æ˜¯å¦ä½¿ç”¨vLLMä½œä¸ºæ¨ç†åç«¯ï¼ˆé«˜ååé‡åœºæ™¯æ¨èï¼‰
            vllm_gpu_memory_utilization: vLLM GPUæ˜¾å­˜å ç”¨æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰
            vllm_tensor_parallel_size: vLLMå¼ é‡å¹¶è¡ŒGPUæ•°é‡
            vllm_max_model_len: vLLMæœ€å¤§åºåˆ—é•¿åº¦
            vllm_dtype: vLLMæ•°æ®ç±»å‹ï¼ˆauto/float16/bfloat16ï¼‰
        """
        self.use_vllm = use_vllm
        self.device = device if torch.cuda.is_available() else "cpu"
        self.model_path = model_path
        self.is_lora = is_lora

        if use_vllm and not VLLM_AVAILABLE:
            raise ImportError(
                "use_vllm=True ä½† vLLM æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œ: pip install vllm"
            )

        # åŠ è½½ç³»ç»Ÿæç¤º
        if system_prompt is None:
            self.system_prompt = load_system_prompt()
            print("âœ“ ä»é…ç½®æ–‡ä»¶åŠ è½½ç³»ç»Ÿæç¤º")
        else:
            self.system_prompt = system_prompt
            print("âœ“ ä½¿ç”¨ä¼ å…¥çš„ç³»ç»Ÿæç¤º")

        print(f"\n{'='*60}")
        print("åˆå§‹åŒ–åŒ»ç–—QAæ¨ç†å™¨")
        print(f"{'='*60}")
        print(f"æ¨ç†åç«¯: {'vLLM' if use_vllm else 'HuggingFace Transformers'}")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"æ˜¯å¦LoRA: {is_lora}")
        if is_lora:
            print(f"åŸºç¡€æ¨¡å‹: {base_model_path}")
            print(f"åˆå¹¶LoRA: {merge_lora}")
        if not use_vllm:
            print(f"è®¾å¤‡: {self.device}")
        print(f"{'='*60}\n")

        if use_vllm:
            self._init_vllm(
                model_path=model_path,
                dtype=vllm_dtype,
                tensor_parallel_size=vllm_tensor_parallel_size,
                gpu_memory_utilization=vllm_gpu_memory_utilization,
                max_model_len=vllm_max_model_len,
            )
        else:
            self._init_hf(
                model_path=model_path,
                base_model_path=base_model_path,
                is_lora=is_lora,
                merge_lora=merge_lora,
                load_in_8bit=load_in_8bit,
                load_in_4bit=load_in_4bit,
            )

    def _init_vllm(
        self,
        model_path: str,
        dtype: str,
        tensor_parallel_size: int,
        gpu_memory_utilization: float,
        max_model_len: int,
    ):
        """ä½¿ç”¨ vLLM åˆå§‹åŒ–æ¨ç†å¼•æ“"""
        print("ğŸ“¥ ä½¿ç”¨ vLLM åŠ è½½æ¨¡å‹...")
        gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
        if gpu_count < tensor_parallel_size:
            print(f"  è­¦å‘Š: å¯ç”¨GPUæ•°({gpu_count}) < è¯·æ±‚å¹¶è¡Œæ•°({tensor_parallel_size})ï¼Œå·²è‡ªåŠ¨è°ƒæ•´")
            tensor_parallel_size = max(1, gpu_count)

        self.vllm_engine = LLM(
            model=model_path,
            dtype=dtype,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=max_model_len,
            trust_remote_code=True,
        )
        self.tokenizer = self.vllm_engine.get_tokenizer()
        print("âœ“ vLLM æ¨¡å‹åŠ è½½å®Œæˆ\n")

    def _init_hf(
        self,
        model_path: str,
        base_model_path: Optional[str],
        is_lora: bool,
        merge_lora: bool,
        load_in_8bit: bool,
        load_in_4bit: bool,
    ):
        """ä½¿ç”¨ HuggingFace Transformers åˆå§‹åŒ–æ¨ç†å¼•æ“"""
        print("ğŸ“¥ åŠ è½½Tokenizer...")
        tokenizer_path = base_model_path if is_lora else model_path
        self.tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            trust_remote_code=True,
            padding_side='left'
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        print("âœ“ TokenizeråŠ è½½å®Œæˆ")

        print("\nğŸ“¥ åŠ è½½æ¨¡å‹...")
        load_kwargs = {
            'trust_remote_code': True,
            'device_map': 'auto' if self.device == 'cuda' else None,
        }
        if not (load_in_8bit or load_in_4bit):
            load_kwargs['torch_dtype'] = torch.bfloat16 if torch.cuda.is_available() else torch.float32
        if load_in_8bit:
            load_kwargs['load_in_8bit'] = True
            print("  ä½¿ç”¨8bité‡åŒ–åŠ è½½")
        elif load_in_4bit:
            load_kwargs['load_in_4bit'] = True
            print("  ä½¿ç”¨4bité‡åŒ–åŠ è½½")

        if is_lora:
            print(f"  åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
            base_model = AutoModelForCausalLM.from_pretrained(base_model_path, **load_kwargs)
            print(f"  åŠ è½½LoRAé€‚é…å™¨: {model_path}")
            model = PeftModel.from_pretrained(base_model, model_path)
            if merge_lora:
                print("  åˆå¹¶LoRAæƒé‡...")
                model = model.merge_and_unload()
                print("  âœ“ LoRAæƒé‡å·²åˆå¹¶")
        else:
            model = AutoModelForCausalLM.from_pretrained(model_path, **load_kwargs)

        model.eval()
        self.model = model
        print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ\n")
    
    def _build_prompt(self, question: str) -> str:
        """å°†é—®é¢˜æ„å»ºä¸ºå¸¦ chat template çš„å®Œæ•´ prompt å­—ç¬¦ä¸²"""
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": question},
        ]
        if hasattr(self.tokenizer, "apply_chat_template"):
            return self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
            )
        # æ‰‹åŠ¨æ„å»ºï¼ˆQwen æ ¼å¼å…œåº•ï¼‰
        text = f"<|im_start|>system\n{self.system_prompt}<|im_end|>\n"
        text += f"<|im_start|>user\n{question}<|im_end|>\n"
        text += "<|im_start|>assistant\n"
        return text

    def generate(
        self,
        question: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        repetition_penalty: float = 1.0,
        do_sample: bool = True,
    ) -> str:
        """
        ç”Ÿæˆå•ä¸ªé—®é¢˜çš„å›ç­”

        Args:
            question: é—®é¢˜
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦ï¼ˆè¶Šé«˜è¶Šéšæœºï¼‰
            top_p: nucleus sampling
            top_k: top-k sampling
            repetition_penalty: é‡å¤æƒ©ç½š
            do_sample: æ˜¯å¦é‡‡æ ·

        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        if self.use_vllm:
            return self._generate_vllm(
                question, max_new_tokens, temperature, top_p, top_k,
                repetition_penalty, do_sample
            )
        return self._generate_hf(
            question, max_new_tokens, temperature, top_p, top_k,
            repetition_penalty, do_sample
        )

    def _generate_vllm(
        self,
        question: str,
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        repetition_penalty: float,
        do_sample: bool,
    ) -> str:
        """ä½¿ç”¨ vLLM ç”Ÿæˆå›ç­”"""
        prompt = self._build_prompt(question)
        sampling_params = SamplingParams(
            max_tokens=max_new_tokens,
            temperature=0.0 if not do_sample else temperature,
            top_p=1.0 if not do_sample else top_p,
            top_k=-1 if not do_sample else top_k,
            repetition_penalty=repetition_penalty,
            stop=["<|im_end|>"],
        )
        outputs = self.vllm_engine.generate(prompt, sampling_params)
        return outputs[0].outputs[0].text.strip()

    def _generate_hf(
        self,
        question: str,
        max_new_tokens: int,
        temperature: float,
        top_p: float,
        top_k: int,
        repetition_penalty: float,
        do_sample: bool,
    ) -> str:
        """ä½¿ç”¨ HuggingFace Transformers ç”Ÿæˆå›ç­”"""
        text = self._build_prompt(question)
        inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                repetition_penalty=repetition_penalty,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
        response = self.tokenizer.decode(generated_ids, skip_special_tokens=True)
        return response.strip()
    
    def batch_generate(
        self,
        questions: List[str],
        batch_size: int = 4,
        **generate_kwargs
    ) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆå›ç­”ã€‚vLLM åç«¯ä¼šä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ promptï¼Œå……åˆ†å‘æŒ¥å¹¶è¡Œååä¼˜åŠ¿ï¼›
        HuggingFace åç«¯æŒ‰ batch_size åˆ†æ‰¹é¡ºåºæ¨ç†ã€‚

        Args:
            questions: é—®é¢˜åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆä»…å¯¹ HuggingFace åç«¯æœ‰æ•ˆï¼‰
            **generate_kwargs: ä¼ é€’ç»™ generate çš„å…¶ä»–å‚æ•°

        Returns:
            å›ç­”åˆ—è¡¨
        """
        if self.use_vllm:
            return self._batch_generate_vllm(questions, **generate_kwargs)
        return self._batch_generate_hf(questions, batch_size, **generate_kwargs)

    def _batch_generate_vllm(self, questions: List[str], **generate_kwargs) -> List[str]:
        """vLLM æ‰¹é‡æ¨ç†ï¼šä¸€æ¬¡æäº¤å…¨éƒ¨ prompt"""
        max_new_tokens = generate_kwargs.get("max_new_tokens", 512)
        temperature = generate_kwargs.get("temperature", 0.7)
        top_p = generate_kwargs.get("top_p", 0.9)
        top_k = generate_kwargs.get("top_k", 50)
        repetition_penalty = generate_kwargs.get("repetition_penalty", 1.0)
        do_sample = generate_kwargs.get("do_sample", True)

        prompts = [self._build_prompt(q) for q in questions]
        sampling_params = SamplingParams(
            max_tokens=max_new_tokens,
            temperature=0.0 if not do_sample else temperature,
            top_p=1.0 if not do_sample else top_p,
            top_k=-1 if not do_sample else top_k,
            repetition_penalty=repetition_penalty,
            stop=["<|im_end|>"],
        )
        outputs = self.vllm_engine.generate(prompts, sampling_params)
        return [output.outputs[0].text.strip() for output in outputs]

    def _batch_generate_hf(
        self,
        questions: List[str],
        batch_size: int,
        **generate_kwargs
    ) -> List[str]:
        """HuggingFace åˆ†æ‰¹æ¨ç†"""
        from tqdm import tqdm

        answers = []
        for i in tqdm(range(0, len(questions), batch_size), desc="æ‰¹é‡æ¨ç†"):
            batch = questions[i:i + batch_size]
            for question in batch:
                answer = self.generate(question, **generate_kwargs)
                answers.append(answer)
        return answers
    
    def interactive_chat(self):
        """äº¤äº’å¼å¯¹è¯æ¨¡å¼"""
        print("\n" + "="*60)
        print("åŒ»ç–—QAäº¤äº’å¼å¯¹è¯")
        print("="*60)
        print("è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("="*60 + "\n")
        
        while True:
            try:
                question = input("\nğŸ‘¤ ç”¨æˆ·: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("\nå†è§ï¼")
                    break
                
                if not question:
                    continue
                
                print("\nğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
                answer = self.generate(question)
                print(answer)
                
            except KeyboardInterrupt:
                print("\n\nå†è§ï¼")
                break
            except Exception as e:
                print(f"\nâŒ é”™è¯¯: {e}")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åŒ»ç–—QAæ¨¡å‹æ¨ç†")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--base_model_path",
        type=str,
        default=None,
        help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆä»…å½“ä½¿ç”¨LoRAæ—¶éœ€è¦ï¼‰"
    )
    parser.add_argument(
        "--is_lora",
        action="store_true",
        help="æ˜¯å¦æ˜¯LoRAæ¨¡å‹"
    )
    parser.add_argument(
        "--no_merge",
        action="store_true",
        help="ä¸åˆå¹¶LoRAï¼ˆé»˜è®¤åˆå¹¶ï¼‰"
    )
    parser.add_argument(
        "--question",
        type=str,
        default=None,
        help="å•ä¸ªé—®é¢˜ï¼ˆå¦‚æœä¸æä¾›ï¼Œè¿›å…¥äº¤äº’æ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--questions_file",
        type=str,
        default=None,
        help="é—®é¢˜æ–‡ä»¶ï¼ˆJSONæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆæ‰¹é‡æ¨ç†æ—¶ï¼‰"
    )
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=512,
        help="æœ€å¤§ç”Ÿæˆtokenæ•°"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="æ¸©åº¦"
    )
    parser.add_argument(
        "--top_p",
        type=float,
        default=0.9,
        help="top_p"
    )
    parser.add_argument(
        "--load_in_8bit",
        action="store_true",
        help="ä»¥8bitåŠ è½½æ¨¡å‹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰"
    )
    parser.add_argument(
        "--load_in_4bit",
        action="store_true",
        help="ä»¥4bitåŠ è½½æ¨¡å‹ï¼ˆæ›´çœæ˜¾å­˜ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    inferencer = MedicalQAInference(
        model_path=args.model_path,
        base_model_path=args.base_model_path,
        is_lora=args.is_lora,
        merge_lora=not args.no_merge,
        load_in_8bit=args.load_in_8bit,
        load_in_4bit=args.load_in_4bit,
    )
    
    # å•ä¸ªé—®é¢˜
    if args.question:
        print(f"\né—®é¢˜: {args.question}\n")
        answer = inferencer.generate(
            args.question,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
        )
        print(f"å›ç­”: {answer}\n")
    
    # æ‰¹é‡é—®é¢˜
    elif args.questions_file:
        print(f"\nä»æ–‡ä»¶åŠ è½½é—®é¢˜: {args.questions_file}")
        
        with open(args.questions_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æ”¯æŒå¤šç§æ ¼å¼
        if isinstance(data, list):
            if isinstance(data[0], str):
                questions = data
            elif isinstance(data[0], dict):
                questions = [item.get('question') or item.get('query') or '' for item in data]
        else:
            raise ValueError("ä¸æ”¯æŒçš„é—®é¢˜æ–‡ä»¶æ ¼å¼")
        
        print(f"å…± {len(questions)} ä¸ªé—®é¢˜\n")
        
        # æ‰¹é‡ç”Ÿæˆ
        answers = inferencer.batch_generate(
            questions,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
        )
        
        # ä¿å­˜ç»“æœ
        results = [
            {"question": q, "answer": a}
            for q, a in zip(questions, answers)
        ]
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        else:
            # æ‰“å°å‰3ä¸ª
            print("\n" + "="*60)
            print("ç¤ºä¾‹ç»“æœï¼ˆå‰3ä¸ªï¼‰:")
            print("="*60)
            for i, result in enumerate(results[:3], 1):
                print(f"\n[{i}] é—®é¢˜: {result['question']}")
                print(f"    å›ç­”: {result['answer'][:200]}...")
    
    # äº¤äº’æ¨¡å¼
    else:
        inferencer.interactive_chat()


if __name__ == "__main__":
    main()
