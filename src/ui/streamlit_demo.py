import streamlit as st
from llmtuner import ChatModel
from llmtuner.extras.misc import torch_gc

from config.config import MODEL_PATH

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰
'''
@st.cache_resource æ˜¯ Streamlit æä¾›çš„ä¸€ä¸ªè£…é¥°å™¨ï¼Œ

å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯ç¼“å­˜èµ„æºå¯†é›†å‹å¯¹è±¡ï¼Œ
ä»¥é¿å…åœ¨æ¯æ¬¡é¡µé¢åˆ·æ–°æˆ–äº¤äº’æ—¶é‡æ–°åˆ›å»ºè¿™äº›å¯¹è±¡ã€‚

ä½¿ç”¨ @st.cache_resource çš„å¥½å¤„åŒ…æ‹¬ï¼š

æ€§èƒ½ä¼˜åŒ–ï¼šæ¨¡å‹åªéœ€åŠ è½½ä¸€æ¬¡ï¼Œåç»­è°ƒç”¨ç›´æ¥ä½¿ç”¨ç¼“å­˜ç»“æœ
èµ„æºèŠ‚çœï¼šé¿å…é‡å¤å ç”¨æ˜¾å­˜å’Œå†…å­˜
'''
@st.cache_resource  
def load_model():  
    return ChatModel(  
        model_name_or_path=MODEL_PATH,  # æŒ‡å‘æœ¬åœ°æƒé‡è·¯å¾„  
        template="chatml",  # Qwen2ä¸“ç”¨å¯¹è¯æ¨¡æ¿  
        trust_remote_code=True,  # å¿…éœ€å‚æ•°  
        load_in_4bit=True,  # 4bité‡åŒ–èŠ‚çœæ˜¾å­˜  
        # å¯é€‰çš„è®¾å¤‡æ˜ å°„é…ç½®  
        # device_map="auto"  
    )  

chat_model = load_model()
INIT_HISTORY = [
            (
            'ç°åœ¨ä½ æ˜¯ä¸€åä¸“ä¸šçš„ä¸­åŒ»åŒ»ç”Ÿï¼Œè¯·ç”¨ä½ çš„ä¸“ä¸šçŸ¥è¯†æä¾›è¯¦å°½è€Œæ¸…æ™°çš„å…³äºä¸­åŒ»é—®é¢˜çš„å›ç­”ã€‚', 
            'å½“ç„¶ï¼Œæˆ‘å°†å°½åŠ›ä¸ºæ‚¨æä¾›å…³äºä¸­åŒ»çš„è¯¦ç»†è€Œæ¸…æ™°çš„å›ç­”ã€‚è¯·é—®æ‚¨æœ‰ç‰¹å®šçš„ä¸­åŒ»é—®é¢˜æˆ–ä¸»é¢˜æ„Ÿå…´è¶£å—ï¼Ÿæ‚¨å¯ä»¥æå‡ºæ‚¨æƒ³äº†è§£çš„ä¸­åŒ»ç›¸å…³é—®é¢˜ï¼Œæ¯”å¦‚ä¸­åŒ»ç†è®ºã€è¯Šæ–­æ–¹æ³•ã€æ²»ç–—æŠ€æœ¯ã€ä¸­è¯ç­‰æ–¹é¢çš„é—®é¢˜ã€‚æˆ‘å°†æ ¹æ®æ‚¨çš„éœ€æ±‚æä¾›ç›¸åº”çš„è§£ç­”ã€‚'
            )
        ]

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "history" not in st.session_state:
    st.session_state.history = INIT_HISTORY.copy()

# é¡µé¢é…ç½®
st.set_page_config(page_title="ğŸ§‘âš•ï¸ ä¸­åŒ»åŠ©æ‰‹")
st.title("ğŸ§‘âš•ï¸ ä¸­åŒ»æ™ºèƒ½åŠ©æ‰‹")
st.caption("è¾“å…¥æ‚¨çš„ä¸­åŒ»é—®é¢˜ï¼Œä½¿ç”¨æ¸…é™¤æŒ‰é’®é‡ç½®å¯¹è¯")

# èŠå¤©å®¹å™¨ ã€æŠŠå†å²è®°å½•å†™å…¥å®¹å™¨å†…ã€‘
chat_container = st.container()
with chat_container:
    for query, response in st.session_state.history[1:]:  # è·³è¿‡åˆå§‹æç¤º (ç¬¬ä¸€è½®å¯¹è¯)
        with st.chat_message("user"):
            st.write(query)
        with st.chat_message("assistant"):
            st.write(response)

# è¾“å…¥åŒºåŸŸ
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # st.session_state.history.append({"role": "user", "content": prompt})  
    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    with chat_container:
        with st.chat_message("user"):
            st.write(prompt)
    
    # ç”Ÿæˆå“åº”
    with chat_container:
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            for chunk in chat_model.stream_chat(
                    prompt, 
                    st.session_state.history,
                    temperature=0.7,
                    repetition_penalty=1.1 
                ):
                full_response += chunk
                response_placeholder.markdown(full_response + "â–Œ")
            
            response_placeholder.markdown(full_response)
    
    # æ›´æ–°å†å²è®°å½•
    st.session_state.history.append((prompt, full_response))

# ä¾§è¾¹æ æ§åˆ¶
with st.sidebar:
    if st.button("ğŸ§¹ æ¸…é™¤å†å²"):
        st.session_state.history = INIT_HISTORY.copy()
        torch_gc()
        st.rerun()
        
        
# streamlit run medical_chat.py  


'''
streamlit run medical_chat.py \
  --server.headless true \
  --browser.gatherUsageStats false \
  --server.port 8080

'''