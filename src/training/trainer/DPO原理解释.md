

1. **背景需求**：
大模型训练最后阶段需要"对齐"（让模型输出符合人类价值观）。传统方法RLHF分三步：
- 训练初始模型
- 收集人类标注训练奖励模型
- 用强化学习（如PPO）优化模型

但这个过程复杂且不稳定，DPO直接跳过奖励模型训练环节。

2. **核心洞察**：
发现奖励模型和策略模型之间存在数学等价关系。通过数学变换，可以把强化学习的优化目标转化为直接比较输出优劣的分类任务。

3. **关键转换**：
假设奖励函数r(x,y)可以表示为：
r(x,y) = β * log(π(y|x)/π_ref(y|x)) 
（π是当前策略【正在微调的模型】，π_ref是参考策略，β是调节参数）

这样就把奖励函数和策略（模型）参数直接挂钩，不再需要单独训练奖励模型。

4. **损失函数设计**：
使用人类标注的偏好数据（回答A比回答B好），构建损失函数：
L = -log(σ(r(x,y_w) - r(x,y_l)))
（y_w是优选回答，y_l是劣选回答，σ是sigmoid函数）

代入之前的奖励函数表达式后，最终损失函数只包含策略模型的参数，实现端到端优化。

5. **训练过程**：
- 输入提示x和一对回答(y_w,y_l)
- 计算两个回答在当前策略和参考策略下的概率比
- 通过sigmoid函数判断哪个更符合人类偏好
- 反向传播直接更新策略参数

6. **效果保障**：
- KL散度约束：通过β参数控制新策略与参考策略的偏离程度
- 隐式正则化：概率比的形式天然防止策略突变

7. **优势体现**：
- 省去奖励模型训练环节
- 训练稳定（类似监督学习）
- 计算资源节省约3倍
- 在对话、摘要等任务中达到或超越RLHF效果

举个例子：当训练聊天机器人时，给定问题"如何做蛋糕"，标注员认为回答A（详细步骤）比回答B（模糊说明）更好。DPO会直接调整模型参数，使得生成A的概率提升，同时保持与原始模型的合理差异。
