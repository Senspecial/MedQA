#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DPOè®­ç»ƒè„šæœ¬ - ä½¿ç”¨TRLåº“
æ”¯æŒä»SFT LoRAæ¨¡å‹å¼€å§‹è®­ç»ƒ
"""

import os
import sys
import json
import yaml
import torch
from pathlib import Path
from typing import Dict, Optional
from dataclasses import dataclass, field

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = Path(__file__).resolve().parent
training_dir = script_dir.parent
src_dir = training_dir.parent
project_root = src_dir.parent
sys.path.insert(0, str(project_root))

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    HfArgumentParser
)
from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training
from trl import DPOTrainer, DPOConfig
from datasets import Dataset, load_dataset
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class ScriptArguments:
    """è„šæœ¬å‚æ•°"""
    config_path: str = field(
        default="config/dpo_training_config.yaml",
        metadata={"help": "é…ç½®æ–‡ä»¶è·¯å¾„"}
    )


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_and_merge_lora_model(
    base_model_path: str,
    lora_checkpoint_path: str,
    merge_lora: bool = True
):
    """
    åŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAæ£€æŸ¥ç‚¹ï¼Œå¯é€‰æ‹©æ˜¯å¦åˆå¹¶
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_checkpoint_path: LoRAæ£€æŸ¥ç‚¹è·¯å¾„
        merge_lora: æ˜¯å¦åˆå¹¶LoRAæƒé‡
        
    Returns:
        model, tokenizer
    """
    logger.info("=" * 60)
    logger.info("åŠ è½½SFTæ¨¡å‹")
    logger.info("=" * 60)
    logger.info(f"åŸºç¡€æ¨¡å‹: {base_model_path}")
    logger.info(f"LoRAæ£€æŸ¥ç‚¹: {lora_checkpoint_path}")
    logger.info(f"åˆå¹¶LoRA: {merge_lora}")
    
    # åŠ è½½tokenizer
    logger.info("\nåŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        padding_side="left"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    logger.info("âœ“ TokenizeråŠ è½½å®Œæˆ")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    logger.info("\nåŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None
    )
    logger.info("âœ“ åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½LoRAé€‚é…å™¨
    logger.info("\nåŠ è½½LoRAé€‚é…å™¨...")
    model = PeftModel.from_pretrained(base_model, lora_checkpoint_path)
    logger.info("âœ“ LoRAé€‚é…å™¨åŠ è½½å®Œæˆ")
    
    if merge_lora:
        # åˆå¹¶LoRAæƒé‡
        logger.info("\nğŸ”€ åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
        model = model.merge_and_unload()
        logger.info("âœ“ LoRAæƒé‡å·²åˆå¹¶")
    else:
        logger.info("\nâš¡ ä¿æŒLoRAé€‚é…å™¨ï¼ˆæœªåˆå¹¶ï¼‰")
    
    logger.info("=" * 60)
    
    return model, tokenizer


def load_dpo_dataset(
    data_path: str, 
    max_length: int = 512,
    max_prompt_length: int = 256,
    system_prompt: Optional[str] = None
) -> Dataset:
    """
    åŠ è½½DPOæ•°æ®é›†ï¼ˆç›´æ¥åŠ è½½ï¼Œä¸ä½¿ç”¨MedicalDatasetçš„tokenizationï¼‰
    
    TRL DPOTrainerä¼šè‡ªå·±å¤„ç†tokenizationï¼Œæˆ‘ä»¬åªéœ€æä¾›åŸå§‹æ–‡æœ¬
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        max_length: æœ€å¤§é•¿åº¦ï¼ˆä¼ ç»™DPOTraineré…ç½®ï¼‰
        max_prompt_length: æœ€å¤§prompté•¿åº¦ï¼ˆä¼ ç»™DPOTraineré…ç½®ï¼‰
        system_prompt: ç³»ç»Ÿæç¤º
        
    Returns:
        Datasetå¯¹è±¡ï¼ˆTRL DPOæ ¼å¼ï¼šprompt, chosen, rejectedï¼‰
    """
    logger.info(f"\nğŸ“‚ åŠ è½½DPOæ•°æ®: {data_path}")
    
    # ç›´æ¥åŠ è½½JSON
    with open(data_path, 'r', encoding='utf-8') as f:
        raw_data = json.load(f)
    
    logger.info(f"åŸå§‹æ•°æ®æ ·æœ¬æ•°: {len(raw_data)}")
    
    # æ ¼å¼åŒ–ä¸ºTRLæ ¼å¼
    formatted_data = []
    system_msg = system_prompt or "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚"
    
    for item in raw_data:
        # ä»DPOæ„é€ æ•°æ®ä¸­æå–å­—æ®µ
        prompt = item.get('prompt', '')
        chosen = item.get('chosen', '')
        rejected = item.get('rejected', '')
        
        if not prompt or not chosen or not rejected:
            logger.warning(f"è·³è¿‡ä¸å®Œæ•´çš„æ ·æœ¬: {item.get('metadata', {}).get('source_id', 'unknown')}")
            continue
        
        # æ„å»ºå®Œæ•´çš„promptï¼ˆTRLæ ¼å¼ï¼‰
        # åŒ…å«ç³»ç»Ÿæç¤º + ç”¨æˆ·é—®é¢˜ + assistantå¼€å§‹æ ‡è®°
        full_prompt = f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        formatted_data.append({
            'prompt': full_prompt,
            'chosen': chosen,  # åªåŒ…å«å›ç­”å†…å®¹
            'rejected': rejected  # åªåŒ…å«å›ç­”å†…å®¹
        })
    
    logger.info(f"âœ“ æœ‰æ•ˆDPOæ ·æœ¬æ•°: {len(formatted_data)}")
    
    # åˆ›å»ºDatasetï¼ˆTRLä¼šè‡ªåŠ¨å¤„ç†tokenizationï¼‰
    dataset = Dataset.from_list(formatted_data)
    
    return dataset


def load_model_and_tokenizer(model_config: Dict, project_root: Path):
    """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆæ”¯æŒä»SFT LoRAå¼€å§‹ï¼‰"""
    
    base_model_path = model_config['base_model_path']
    if not os.path.isabs(base_model_path):
        base_model_path = os.path.join(project_root, base_model_path)
    
    is_sft_lora = model_config.get('is_lora', False)
    sft_checkpoint = model_config.get('sft_checkpoint_path')
    
    # å¦‚æœSFTæ¨¡å‹æ˜¯LoRAï¼Œå…ˆåŠ è½½å¹¶åˆå¹¶
    if is_sft_lora and sft_checkpoint:
        if not os.path.isabs(sft_checkpoint):
            sft_checkpoint = os.path.join(project_root, sft_checkpoint)
        
        logger.info("\nğŸ“¥ åŠ è½½SFT LoRAæ¨¡å‹...")
        model, tokenizer = load_and_merge_lora_model(
            base_model_path=base_model_path,
            lora_checkpoint_path=sft_checkpoint,
            merge_lora=True  # åˆå¹¶SFT LoRAï¼Œç„¶ååœ¨å…¶åŸºç¡€ä¸Šè®­ç»ƒDPO
        )
        
        # è°ƒæ•´tokenizerè®¾ç½®ï¼ˆDPOè®­ç»ƒéœ€è¦ï¼‰
        tokenizer.padding_side = "right"  # DPOè®­ç»ƒä½¿ç”¨right padding
        
    else:
        # æ²¡æœ‰SFT checkpointæˆ–ä¸æ˜¯LoRAï¼Œç›´æ¥åŠ è½½åŸºç¡€æ¨¡å‹
        logger.info(f"\nğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
        logger.info(f"  æ¨¡å‹è·¯å¾„: {base_model_path}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            padding_side="right"  # DPOè®­ç»ƒä½¿ç”¨right padding
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id
        
        model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            use_cache=False  # DPOè®­ç»ƒæ—¶ç¦ç”¨cache
        )
        
        logger.info("âœ“ åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # ç¦ç”¨æ¨¡å‹ç¼“å­˜ï¼ˆDPOè®­ç»ƒè¦æ±‚ï¼‰
    model.config.use_cache = False
    
    return model, tokenizer


def setup_lora_config(lora_config: Dict) -> Optional[LoraConfig]:
    """é…ç½®LoRA"""
    if not lora_config.get('enabled', False):
        return None
    
    logger.info("\nâš™ï¸ é…ç½®LoRA...")
    
    config = LoraConfig(
        r=lora_config.get('r', 8),
        lora_alpha=lora_config.get('lora_alpha', 32),
        target_modules=lora_config.get('target_modules', ["q_proj", "v_proj"]),
        lora_dropout=lora_config.get('lora_dropout', 0.05),
        bias=lora_config.get('bias', "none"),
        task_type="CAUSAL_LM"
    )
    
    logger.info(f"  LoRA rank: {config.r}")
    logger.info(f"  LoRA alpha: {config.lora_alpha}")
    logger.info(f"  Target modules: {config.target_modules}")
    logger.info("âœ“ LoRAé…ç½®å®Œæˆ")
    
    return config


def train_dpo(config_path: str):
    """æ‰§è¡ŒDPOè®­ç»ƒ"""
    
    # åŠ è½½é…ç½®
    logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    config = load_config(config_path)
    
    # æå–é…ç½®
    model_config = config['model']
    data_config = config['data']
    training_config = config['training']
    lora_config = config.get('lora', {})
    output_config = config['output']
    
    # è·¯å¾„å¤„ç†
    train_data_path = data_config['train_data_path']
    if not os.path.isabs(train_data_path):
        train_data_path = os.path.join(project_root, train_data_path)
    
    eval_data_path = data_config.get('eval_data_path')
    if eval_data_path and not os.path.isabs(eval_data_path):
        eval_data_path = os.path.join(project_root, eval_data_path)
    
    output_dir = output_config['output_dir']
    if not os.path.isabs(output_dir):
        output_dir = os.path.join(project_root, output_dir)
    
    print("\n" + "=" * 70)
    print("DPOè®­ç»ƒé…ç½®")
    print("=" * 70)
    print(f"åŸºç¡€æ¨¡å‹: {model_config['base_model_path']}")
    if model_config.get('sft_checkpoint_path'):
        print(f"SFTæ£€æŸ¥ç‚¹: {model_config['sft_checkpoint_path']}")
    print(f"è®­ç»ƒæ•°æ®: {train_data_path}")
    if eval_data_path:
        print(f"è¯„ä¼°æ•°æ®: {eval_data_path}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ä½¿ç”¨LoRA: {lora_config.get('enabled', False)}")
    print(f"è®­ç»ƒè½®æ•°: {training_config['num_train_epochs']}")
    print(f"æ‰¹æ¬¡å¤§å°: {training_config['per_device_train_batch_size']}")
    print(f"å­¦ä¹ ç‡: {training_config['learning_rate']}")
    print(f"Beta (DPO): {training_config.get('beta', 0.1)}")
    print("=" * 70)
    
    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = load_model_and_tokenizer(model_config, project_root)
    
    # 2. é…ç½®LoRAï¼ˆå¦‚æœéœ€è¦ï¼‰
    peft_config = setup_lora_config(lora_config)
    model_is_peft = False  # è·Ÿè¸ªæ¨¡å‹æ˜¯å¦å·²ç»æ˜¯PeftModel
    
    if peft_config:
        if not isinstance(model, PeftModel):
            # æ¨¡å‹è¿˜ä¸æ˜¯PeftModelï¼ŒDPOTrainerä¼šåº”ç”¨peft_config
            logger.info("âœ“ å°†åœ¨DPOè®­ç»ƒä¸­åº”ç”¨æ–°çš„LoRAé…ç½®")
            model_is_peft = False
        else:
            # æ¨¡å‹å·²ç»æ˜¯PeftModelï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæˆ‘ä»¬åˆå¹¶äº†ï¼‰
            logger.warning("âš ï¸ æ¨¡å‹å·²ç»æ˜¯PeftModelï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿ")
            model_is_peft = True
    
    # 3. åŠ è½½æ•°æ®é›†
    logger.info("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    system_prompt = config.get('system_prompt')
    
    train_dataset = load_dpo_dataset(
        train_data_path,
        max_length=data_config.get('max_length', 512),
        max_prompt_length=data_config.get('max_prompt_length', 256),
        system_prompt=system_prompt
    )
    
    eval_dataset = None
    if eval_data_path:
        eval_dataset = load_dpo_dataset(
            eval_data_path,
            max_length=data_config.get('max_length', 512),
            max_prompt_length=data_config.get('max_prompt_length', 256),
            system_prompt=system_prompt
        )
        logger.info(f"è¯„ä¼°é›†æ ·æœ¬æ•°: {len(eval_dataset)}")
    
    # 4. é…ç½®è®­ç»ƒå‚æ•°
    logger.info("\nâš™ï¸ é…ç½®è®­ç»ƒå‚æ•°...")
    
    training_args = DPOConfig(
        output_dir=output_dir,
        
        # åŸºç¡€è®­ç»ƒå‚æ•°
        num_train_epochs=training_config['num_train_epochs'],
        per_device_train_batch_size=training_config['per_device_train_batch_size'],
        per_device_eval_batch_size=training_config.get('per_device_eval_batch_size', 4),
        gradient_accumulation_steps=training_config.get('gradient_accumulation_steps', 1),
        
        # ä¼˜åŒ–å™¨å‚æ•°
        learning_rate=training_config['learning_rate'],
        weight_decay=training_config.get('weight_decay', 0.01),
        warmup_ratio=training_config.get('warmup_ratio', 0.1),
        max_grad_norm=training_config.get('max_grad_norm', 1.0),
        
        # DPOç‰¹å®šå‚æ•°
        beta=training_config.get('beta', 0.1),
        loss_type=training_config.get('loss_type', 'sigmoid'),  # 'sigmoid' or 'hinge' or 'ipo'
        
        # é•¿åº¦é™åˆ¶å‚æ•°
        max_length=data_config.get('max_length', 512),
        max_prompt_length=data_config.get('max_prompt_length', 256),
        
        # è¯„ä¼°å‚æ•°
        eval_strategy=training_config.get('eval_strategy', 'steps'),
        eval_steps=training_config.get('eval_steps', 100),
        
        # ä¿å­˜å‚æ•°
        save_strategy=training_config.get('save_strategy', 'steps'),
        save_steps=training_config.get('save_steps', 200),
        save_total_limit=training_config.get('save_total_limit', 3),
        
        # æ—¥å¿—å‚æ•°
        logging_steps=training_config.get('logging_steps', 10),
        logging_dir=os.path.join(output_dir, 'logs'),
        report_to=training_config.get('report_to', ['tensorboard']),
        
        # å…¶ä»–å‚æ•°
        fp16=torch.cuda.is_available(),
        remove_unused_columns=False,
        gradient_checkpointing=training_config.get('gradient_checkpointing', False),
        
        # æ•°æ®åŠ è½½
        dataloader_num_workers=training_config.get('dataloader_num_workers', 4),
        dataloader_pin_memory=True,
    )
    
    # 5. åˆ›å»ºDPO Trainer
    logger.info("\nğŸš€ åˆ›å»ºDPO Trainer...")
    
    # åªæœ‰å½“æ¨¡å‹ä¸æ˜¯PeftModelæ—¶ï¼Œæ‰ä¼ é€’peft_config
    # DPOTrainerä¼šè‡ªåŠ¨åº”ç”¨peft_configåˆ°åŸºç¡€æ¨¡å‹
    trainer_peft_config = peft_config if not model_is_peft else None
    
    if trainer_peft_config:
        logger.info("  ä½¿ç”¨LoRAè®­ç»ƒï¼ˆDPOTrainerå°†åº”ç”¨peft_configï¼‰")
    else:
        logger.info("  ä½¿ç”¨å…¨å‚æ•°è®­ç»ƒ")
    
    trainer = DPOTrainer(
        model=model,
        ref_model=None,  # Noneè¡¨ç¤ºä½¿ç”¨frozen copyä½œä¸ºreference model
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        processing_class=tokenizer,  # æ–°ç‰ˆTRLä½¿ç”¨processing_classè€Œä¸æ˜¯tokenizer
        peft_config=trainer_peft_config,  # åªæœ‰å½“æ¨¡å‹ä¸æ˜¯PeftModelæ—¶æ‰ä¼ é€’
    )
    
    logger.info("âœ“ DPO Traineråˆ›å»ºå®Œæˆ")
    
    # 6. å¼€å§‹è®­ç»ƒ
    logger.info("\n" + "=" * 70)
    logger.info("å¼€å§‹DPOè®­ç»ƒ...")
    logger.info("=" * 70 + "\n")
    
    train_result = trainer.train()
    
    # 7. ä¿å­˜æ¨¡å‹
    logger.info("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆåŒ…å«SFT+DPOï¼‰
    save_merged = model_config.get('save_merged_dpo', True)
    
    if save_merged and isinstance(model, PeftModel):
        # æƒ…å†µ1: æ¨¡å‹æ˜¯PeftModelï¼ˆæœ‰DPO LoRAï¼‰
        logger.info("åˆå¹¶å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆåŸºç¡€+SFT+DPOï¼‰...")
        
        # åˆå¹¶DPO LoRAåˆ°å·²åŒ…å«SFTçš„æ¨¡å‹
        merged_model = model.merge_and_unload()
        merged_model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        logger.info(f"âœ“ å®Œæ•´åˆå¹¶æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
        # å¦å¤–ä¿å­˜LoRAé€‚é…å™¨åˆ°å­ç›®å½•ï¼ˆå¯é€‰ï¼Œç”¨äºç»§ç»­è®­ç»ƒï¼‰
        lora_dir = os.path.join(output_dir, "dpo_lora_adapter")
        os.makedirs(lora_dir, exist_ok=True)
        model.save_pretrained(lora_dir)
        logger.info(f"âœ“ DPO LoRAé€‚é…å™¨å·²ä¿å­˜åˆ°: {lora_dir}")
        
    else:
        # æƒ…å†µ2: ä½¿ç”¨å…¨å‚æ•°è®­ç»ƒæˆ–å·²ç»æ˜¯å®Œæ•´æ¨¡å‹
        trainer.save_model(output_dir)
        tokenizer.save_pretrained(output_dir)
        logger.info(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    # ä¿å­˜è®­ç»ƒçŠ¶æ€
    trainer.save_state()
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    logger.info(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    # 8. æ‰“å°è®­ç»ƒç»Ÿè®¡
    logger.info("\n" + "=" * 70)
    logger.info("è®­ç»ƒå®Œæˆç»Ÿè®¡")
    logger.info("=" * 70)
    logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    if eval_dataset:
        logger.info(f"è¯„ä¼°æ ·æœ¬æ•°: {len(eval_dataset)}")
    logger.info(f"è®­ç»ƒè½®æ•°: {training_config['num_train_epochs']}")
    logger.info(f"æ€»æ­¥æ•°: {train_result.global_step}")
    logger.info(f"è®­ç»ƒæŸå¤±: {metrics.get('train_loss', 'N/A')}")
    logger.info("=" * 70)
    
    logger.info("\nâœ… DPOè®­ç»ƒå®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    parser = HfArgumentParser(ScriptArguments)
    args = parser.parse_args_into_dataclasses()[0]
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = args.config_path
    if not os.path.isabs(config_path):
        config_path = os.path.join(project_root, config_path)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(config_path):
        logger.error(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        sys.exit(1)
    
    # è¿è¡Œè®­ç»ƒ
    try:
        train_dpo(config_path)
    except Exception as e:
        logger.error(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
#python src/training/scripts/run_dpo_training.py --config_path config/dpo_training_config.yaml