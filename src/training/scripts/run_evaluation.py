#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä½¿ç”¨é…ç½®æ–‡ä»¶è¿è¡ŒSFTæ¨¡å‹è¯„ä¼°
"""

import os
import sys
import json
import yaml
import torch
from pathlib import Path
from typing import Dict, List
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = Path(__file__).resolve().parent
training_dir = script_dir.parent
src_dir = training_dir.parent
project_root = src_dir.parent
sys.path.insert(0, str(project_root))

from transformers import AutoModelForCausalLM, AutoTokenizer
from src.training.dataset.data_processor import MedicalQAEvaluator
import logging

logger = logging.getLogger(__name__)


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def evaluate_with_config(config_path: str):
    """ä½¿ç”¨é…ç½®æ–‡ä»¶è¿›è¡Œè¯„ä¼°"""
    
    # åŠ è½½é…ç½®
    print(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    config = load_config(config_path)
    
    # æå–é…ç½®
    model_path = config['model']['model_path']
    device = config['model']['device']
    
    test_data_path = config['test_data']['test_data_path']
    num_samples = config['test_data'].get('num_samples')
    random_seed = config['test_data'].get('random_seed', 42)
    
    output_dir = config['output']['output_dir']
    
    gen_config = config['generation']
    eval_config = config['evaluation_metrics']
    
    # è·å–APIå¯†é’¥
    api_key = os.environ.get('DEEPSEEK_API_KEY') or config['evaluation_metrics']['judge_model'].get('api_key', '')
    
    print("\n" + "=" * 70)
    print("SFTæ¨¡å‹è¯„ä¼°é…ç½®")
    print("=" * 70)
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"æµ‹è¯•æ•°æ®: {test_data_path}")
    print(f"è¯„ä¼°æ ·æœ¬æ•°: {num_samples or 'å…¨éƒ¨'}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ä½¿ç”¨è¯„å®¡æ¨¡å‹: {eval_config.get('use_judge_model', False)}")
    print(f"è®¡ç®—å›°æƒ‘åº¦: {eval_config.get('calculate_perplexity', False)}")
    print("=" * 70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. åŠ è½½æ¨¡å‹
    print(f"\nğŸ“¥ åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
        padding_side="left"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None
    )
    model.eval()
    print("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 2. åŠ è½½æµ‹è¯•æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
    with open(test_data_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_data)}")
    
    # é‡‡æ ·
    import random
    random.seed(random_seed)
    
    if num_samples and num_samples < len(test_data):
        test_samples = random.sample(test_data, num_samples)
        print(f"éšæœºæŠ½æ · {num_samples} ä¸ªæ ·æœ¬")
    else:
        test_samples = test_data
    
    # 3. ç”Ÿæˆå›ç­”
    print(f"\nğŸ¤– ç”Ÿæˆå›ç­”...")
    
    system_prompt = config.get('system_prompt', 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚')
    
    def generate_response(question: str) -> str:
        """ç”Ÿæˆå›ç­”"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ]
        
        if hasattr(tokenizer, "apply_chat_template"):
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            text = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = tokenizer(text, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=gen_config['max_new_tokens'],
                temperature=gen_config['temperature'],
                top_p=gen_config['top_p'],
                do_sample=gen_config['do_sample'],
                repetition_penalty=gen_config.get('repetition_penalty', 1.0),
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
        response = tokenizer.decode(generated_ids, skip_special_tokens=True)
        
        return response.strip()
    
    # ç”Ÿæˆå›ç­”
    generated_samples = []
    for sample in tqdm(test_samples, desc="ç”Ÿæˆå›ç­”"):
        question = sample.get('question') or sample.get('query') or ""
        ground_truth = sample.get('answer') or sample.get('response') or ""
        
        generated_answer = generate_response(question)
        
        generated_samples.append({
            'question': question,
            'answer': generated_answer,
            'ground_truth': ground_truth,
            'id': sample.get('id', ''),
            'primary_label': sample.get('primary_label', '')
        })
    
    print(f"âœ“ å·²ç”Ÿæˆ {len(generated_samples)} ä¸ªå›ç­”")
    
    # ä¿å­˜ç”Ÿæˆæ ·æœ¬
    if config['output'].get('save_generated_samples', True):
        samples_path = os.path.join(output_dir, 'generated_samples.json')
        with open(samples_path, 'w', encoding='utf-8') as f:
            json.dump(generated_samples, f, ensure_ascii=False, indent=2)
        print(f"âœ“ ç”Ÿæˆæ ·æœ¬å·²ä¿å­˜: {samples_path}")
    
    results = {}
    
    # 4. è®¡ç®—å›°æƒ‘åº¦ï¼ˆå¯é€‰ï¼‰
    if eval_config.get('calculate_perplexity', False):
        print(f"\nğŸ“Š è®¡ç®—å›°æƒ‘åº¦...")
        ppl_samples = test_samples[:eval_config.get('ppl_max_samples')] if eval_config.get('ppl_max_samples') else test_samples
        
        total_loss = 0
        total_tokens = 0
        
        for sample in tqdm(ppl_samples, desc="è®¡ç®—PPL"):
            question = sample.get('question') or ""
            answer = sample.get('answer') or ""
            
            text = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n{answer}<|im_end|>"
            
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(device)
            
            with torch.no_grad():
                outputs = model(**inputs, labels=inputs["input_ids"])
                loss = outputs.loss
                
                total_loss += loss.item() * inputs["input_ids"].size(1)
                total_tokens += inputs["input_ids"].size(1)
        
        import numpy as np
        avg_loss = total_loss / total_tokens
        perplexity = np.exp(avg_loss)
        
        results['perplexity'] = float(perplexity)
        results['avg_loss'] = float(avg_loss)
        
        print(f"âœ“ å›°æƒ‘åº¦: {perplexity:.2f}")
        print(f"  å¹³å‡Loss: {avg_loss:.4f}")
        
        # è¯„ä»·
        quality_standards = config.get('quality_standards', {})
        excellent_ppl = quality_standards.get('excellent_ppl', 15.0)
        good_ppl = quality_standards.get('good_ppl', 30.0)
        acceptable_ppl = quality_standards.get('acceptable_ppl', 50.0)
        
        if perplexity < excellent_ppl:
            print(f"  è¯„ä»·: âœ… ä¼˜ç§€ (< {excellent_ppl})")
        elif perplexity < good_ppl:
            print(f"  è¯„ä»·: âš ï¸ è‰¯å¥½ ({excellent_ppl}-{good_ppl})")
        elif perplexity < acceptable_ppl:
            print(f"  è¯„ä»·: âš ï¸ å¯æ¥å— ({good_ppl}-{acceptable_ppl})")
        else:
            print(f"  è¯„ä»·: âŒ éœ€è¦æ”¹è¿› (> {acceptable_ppl})")
    
    # 5. ä½¿ç”¨è¯„å®¡æ¨¡å‹è¯„ä¼°ï¼ˆå¯é€‰ï¼‰
    if eval_config.get('use_judge_model', False):
        if not api_key:
            print("\nâš ï¸ æœªè®¾ç½®APIå¯†é’¥ï¼Œè·³è¿‡è¯„å®¡æ¨¡å‹è¯„ä¼°")
        else:
            print(f"\nğŸ“Š ä½¿ç”¨è¯„å®¡æ¨¡å‹è¯„ä¼°...")
            
            evaluator = MedicalQAEvaluator(
                api_key=api_key,
                base_url=eval_config['judge_model']['base_url']
            )
            
            # æ‰¹é‡è¯„ä¼°
            evaluated_samples = evaluator.batch_evaluate(
                generated_samples,
                batch_size=eval_config['judge_model']['batch_size'],
                max_workers=eval_config['judge_model']['max_workers']
            )
            
            # ç”ŸæˆæŠ¥å‘Š
            if config['output'].get('save_evaluation_report', True):
                report_path = os.path.join(output_dir, 'evaluation_report.json')
                report = evaluator.generate_evaluation_report(evaluated_samples, report_path)
                
                # æå–ç»Ÿè®¡ä¿¡æ¯
                if 'statistics' in report:
                    stats = report['statistics']
                    results['judge_evaluation'] = stats
                    
                    print(f"\nè¯„ä¼°ç»“æœ:")
                    print(f"  é€šè¿‡ç‡: {stats.get('pass_rate', 0):.1f}%")
                    print(f"  è¶Šæƒç‡: {stats.get('overreach_rate', 0):.1f}%")
                    print(f"  å¹»è§‰ç‡: {stats.get('hallucination_rate', 0):.1f}%")
                    print(f"  çº¢æ——é—æ¼ç‡: {stats.get('red_flag_omission_rate', 0):.1f}%")
                    
                    # ä¸è´¨é‡æ ‡å‡†å¯¹æ¯”
                    quality_standards = config.get('quality_standards', {})
                    print(f"\nè´¨é‡æ ‡å‡†æ£€æŸ¥:")
                    
                    pass_rate = stats.get('pass_rate', 0) / 100
                    if pass_rate >= quality_standards.get('min_pass_rate', 0.7):
                        print(f"  âœ… é€šè¿‡ç‡è¾¾æ ‡ ({pass_rate*100:.1f}% >= {quality_standards.get('min_pass_rate', 0.7)*100:.1f}%)")
                    else:
                        print(f"  âŒ é€šè¿‡ç‡ä¸è¾¾æ ‡ ({pass_rate*100:.1f}% < {quality_standards.get('min_pass_rate', 0.7)*100:.1f}%)")
                    
                    overreach_rate = stats.get('overreach_rate', 0) / 100
                    if overreach_rate <= quality_standards.get('max_overreach_rate', 0.1):
                        print(f"  âœ… è¶Šæƒç‡åˆæ ¼ ({overreach_rate*100:.1f}% <= {quality_standards.get('max_overreach_rate', 0.1)*100:.1f}%)")
                    else:
                        print(f"  âŒ è¶Šæƒç‡è¿‡é«˜ ({overreach_rate*100:.1f}% > {quality_standards.get('max_overreach_rate', 0.1)*100:.1f}%)")
                    
                    hallucination_rate = stats.get('hallucination_rate', 0) / 100
                    if hallucination_rate <= quality_standards.get('max_hallucination_rate', 0.15):
                        print(f"  âœ… å¹»è§‰ç‡åˆæ ¼ ({hallucination_rate*100:.1f}% <= {quality_standards.get('max_hallucination_rate', 0.15)*100:.1f}%)")
                    else:
                        print(f"  âŒ å¹»è§‰ç‡è¿‡é«˜ ({hallucination_rate*100:.1f}% > {quality_standards.get('max_hallucination_rate', 0.15)*100:.1f}%)")
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            if config['output'].get('save_detailed_results', True):
                detailed_path = os.path.join(output_dir, 'detailed_results.json')
                with open(detailed_path, 'w', encoding='utf-8') as f:
                    json.dump(evaluated_samples, f, ensure_ascii=False, indent=2)
                print(f"\nâœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {detailed_path}")
    
    # 6. ä¿å­˜æ€»ç»“
    summary_path = os.path.join(output_dir, 'evaluation_summary.json')
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump({
            'config': config,
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*70}")
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"  - evaluation_summary.json: è¯„ä¼°æ€»ç»“")
    if eval_config.get('use_judge_model'):
        print(f"  - evaluation_report.json: è¯¦ç»†è¯„ä¼°æŠ¥å‘Š")
        print(f"  - detailed_results.json: è¯¦ç»†è¯„ä¼°ç»“æœ")
    if config['output'].get('save_generated_samples'):
        print(f"  - generated_samples.json: ç”Ÿæˆæ ·æœ¬")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨é…ç½®æ–‡ä»¶è¯„ä¼°SFTæ¨¡å‹")
    parser.add_argument(
        "--config",
        type=str,
        default="config/evaluation_config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = args.config
    if not os.path.isabs(config_path):
        config_path = os.path.join(project_root, config_path)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(config_path):
        print(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    # è¿è¡Œè¯„ä¼°
    try:
        evaluate_with_config(config_path)
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
#python src/training/scripts/run_evaluation.py --config config/evaluation_config.yaml
#python src/training/scripts/run_evaluation.py --config config/dpo_evaluation_config.yaml