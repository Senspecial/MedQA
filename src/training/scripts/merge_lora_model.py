#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åˆå¹¶LoRAé€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹
"""

import os
import sys
import torch
import argparse
from pathlib import Path
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = Path(__file__).resolve().parent
training_dir = script_dir.parent
src_dir = training_dir.parent
project_root = src_dir.parent
sys.path.insert(0, str(project_root))

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


def merge_lora_model(
    base_model_path: str,
    lora_adapter_path: str,
    output_path: str,
    device: str = "cuda",
    max_shard_size: str = "5GB"
):
    """
    åˆå¹¶LoRAé€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹
    
    Args:
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        lora_adapter_path: LoRAé€‚é…å™¨è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
        device: è®¾å¤‡
        max_shard_size: æœ€å¤§åˆ†ç‰‡å¤§å°
    """
    
    print("=" * 70)
    print("LoRA æ¨¡å‹åˆå¹¶å·¥å…·")
    print("=" * 70)
    print(f"åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"LoRAé€‚é…å™¨: {lora_adapter_path}")
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"è®¾å¤‡: {device}")
    print("=" * 70)
    
    # 1. åŠ è½½tokenizer
    print("\nğŸ“¥ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True
    )
    print("âœ“ tokenizeråŠ è½½å®Œæˆ")
    
    # 2. åŠ è½½åŸºç¡€æ¨¡å‹
    print("\nğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() and device == "cuda" else torch.float32,
        device_map="auto" if torch.cuda.is_available() and device == "cuda" else None,
        low_cpu_mem_usage=True
    )
    print("âœ“ åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # 3. åŠ è½½LoRAé€‚é…å™¨
    print("\nğŸ“¥ åŠ è½½LoRAé€‚é…å™¨...")
    model = PeftModel.from_pretrained(
        base_model,
        lora_adapter_path,
        torch_dtype=torch.float16 if torch.cuda.is_available() and device == "cuda" else torch.float32
    )
    print("âœ“ LoRAé€‚é…å™¨åŠ è½½å®Œæˆ")
    
    # 4. åˆå¹¶æƒé‡
    print("\nğŸ”€ åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
    merged_model = model.merge_and_unload()
    print("âœ“ æƒé‡åˆå¹¶å®Œæˆ")
    
    # 5. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
    os.makedirs(output_path, exist_ok=True)
    
    merged_model.save_pretrained(
        output_path,
        max_shard_size=max_shard_size,
        safe_serialization=True
    )
    print("âœ“ æ¨¡å‹å·²ä¿å­˜")
    
    # 6. ä¿å­˜tokenizer
    print(f"\nğŸ’¾ ä¿å­˜tokenizer...")
    tokenizer.save_pretrained(output_path)
    print("âœ“ tokenizerå·²ä¿å­˜")
    
    # 7. ä¿å­˜é…ç½®ä¿¡æ¯
    print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶ä¿¡æ¯...")
    merge_info = {
        "base_model": base_model_path,
        "lora_adapter": lora_adapter_path,
        "merged_at": str(Path(output_path).absolute()),
        "device": device,
        "dtype": str(merged_model.dtype)
    }
    
    import json
    with open(os.path.join(output_path, "merge_info.json"), 'w', encoding='utf-8') as f:
        json.dump(merge_info, f, ensure_ascii=False, indent=2)
    print("âœ“ åˆå¹¶ä¿¡æ¯å·²ä¿å­˜")
    
    print("\n" + "=" * 70)
    print("âœ… LoRAæ¨¡å‹åˆå¹¶å®Œæˆï¼")
    print("=" * 70)
    print(f"\nåˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨: {output_path}")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("```python")
    print("from transformers import AutoModelForCausalLM, AutoTokenizer")
    print(f'model = AutoModelForCausalLM.from_pretrained("{output_path}")')
    print(f'tokenizer = AutoTokenizer.from_pretrained("{output_path}")')
    print("```")
    
    # æ¸…ç†å†…å­˜
    del merged_model
    del model
    del base_model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶LoRAé€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹")
    
    parser.add_argument(
        "--base_model",
        type=str,
        required=True,
        help="åŸºç¡€æ¨¡å‹è·¯å¾„"
    )
    
    parser.add_argument(
        "--lora_adapter",
        type=str,
        required=True,
        help="LoRAé€‚é…å™¨è·¯å¾„"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¾“å‡ºè·¯å¾„"
    )
    
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        choices=["cuda", "cpu"],
        help="è®¾å¤‡"
    )
    
    parser.add_argument(
        "--max_shard_size",
        type=str,
        default="5GB",
        help="æœ€å¤§åˆ†ç‰‡å¤§å°ï¼ˆå¦‚ï¼š5GB, 2GBï¼‰"
    )
    
    args = parser.parse_args()
    
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    base_model_path = args.base_model
    if not os.path.isabs(base_model_path):
        base_model_path = os.path.join(project_root, base_model_path)
    
    lora_adapter_path = args.lora_adapter
    if not os.path.isabs(lora_adapter_path):
        lora_adapter_path = os.path.join(project_root, lora_adapter_path)
    
    output_path = args.output
    if not os.path.isabs(output_path):
        output_path = os.path.join(project_root, output_path)
    
    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(base_model_path):
        print(f"âŒ é”™è¯¯: åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}")
        sys.exit(1)
    
    if not os.path.exists(lora_adapter_path):
        print(f"âŒ é”™è¯¯: LoRAé€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {lora_adapter_path}")
        sys.exit(1)
    
    # æ‰§è¡Œåˆå¹¶
    try:
        merge_lora_model(
            base_model_path=base_model_path,
            lora_adapter_path=lora_adapter_path,
            output_path=output_path,
            device=args.device,
            max_shard_size=args.max_shard_size
        )
    except Exception as e:
        print(f"\nâŒ åˆå¹¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
