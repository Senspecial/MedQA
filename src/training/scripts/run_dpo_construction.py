#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DPOè´Ÿæ ·æœ¬æ„é€ è„šæœ¬
ä»SFTæ•°æ®ç”ŸæˆDPOè®­ç»ƒæ‰€éœ€çš„chosen/rejectedå¯¹
"""

import os
import sys
import json
import yaml
import torch
import random
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
from dataclasses import dataclass, asdict
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = Path(__file__).resolve().parent
training_dir = script_dir.parent
src_dir = training_dir.parent
project_root = src_dir.parent
sys.path.insert(0, str(project_root))

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from src.training.dataset.dpo_negative_constructor import (
    ResponseCandidate,
    DPOSample,
    JudgeModel
)

# å°è¯•å¯¼å…¥ vLLM
try:
    from vllm import LLM, SamplingParams
    from vllm.lora.request import LoRARequest
    VLLM_AVAILABLE = True
except ImportError:
    VLLM_AVAILABLE = False

import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class GenerationStats:
    """ç”Ÿæˆç»Ÿè®¡"""
    total_samples: int = 0
    generated_samples: int = 0
    valid_pairs: int = 0
    invalid_pairs: int = 0
    skipped_samples: int = 0
    avg_candidates_per_sample: float = 0.0
    avg_score_difference: float = 0.0


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_sft_model(model_config: Dict, project_root: Path):
    """åŠ è½½SFTæ¨¡å‹"""
    model_path = model_config['model_path']
    if not os.path.isabs(model_path):
        model_path = os.path.join(project_root, model_path)
    
    is_lora = model_config.get('is_lora', False)
    device = model_config['device']
    
    if is_lora:
        base_model_path = model_config.get('base_model_path')
        if not os.path.isabs(base_model_path):
            base_model_path = os.path.join(project_root, base_model_path)
        
        logger.info(f"åŠ è½½LoRAæ¨¡å‹...")
        logger.info(f"  åŸºç¡€æ¨¡å‹: {base_model_path}")
        logger.info(f"  LoRAé€‚é…å™¨: {model_path}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            padding_side="left"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        # åŠ è½½LoRAé€‚é…å™¨
        model = PeftModel.from_pretrained(base_model, model_path)
        model.eval()
        
        logger.info("âœ“ LoRAæ¨¡å‹åŠ è½½å®Œæˆ")
    else:
        logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="left"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        model.eval()
        
        logger.info("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    return model, tokenizer


def load_sft_model_vllm(
    model_config: Dict, vllm_config: Dict, project_root: Path
):
    """
    ä½¿ç”¨ vLLM åŠ è½½ SFT æ¨¡å‹ï¼Œæ”¯æŒå®Œæ•´æ¨¡å‹å’Œ LoRA é€‚é…å™¨ä¸¤ç§å½¢å¼ã€‚

    - å®Œæ•´æ¨¡å‹ï¼ˆis_lora=Falseï¼‰ï¼šç›´æ¥åŠ è½½
    - LoRA æ¨¡å‹ï¼ˆis_lora=Trueï¼‰ï¼šåŠ è½½ base model å¹¶å¯ç”¨ enable_loraï¼Œ
      è¿”å› LoRARequest ä¾›ç”Ÿæˆæ—¶ä½¿ç”¨

    Returns:
        (engine, tokenizer, lora_request_or_None)
    """
    if not VLLM_AVAILABLE:
        raise ImportError("vLLM æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œ: pip install vllm")

    is_lora = model_config.get('is_lora', False)

    # è§£æè·¯å¾„
    model_path = model_config['model_path']
    if not os.path.isabs(model_path):
        model_path = os.path.join(project_root, model_path)

    if is_lora:
        base_model_path = model_config.get('base_model_path', '')
        if not os.path.isabs(base_model_path):
            base_model_path = os.path.join(project_root, base_model_path)
        load_path = base_model_path  # vLLM åŠ è½½ base model
        lora_path = model_path       # LoRA é€‚é…å™¨è·¯å¾„
        logger.info(f"vLLM LoRA æ¨¡å¼ â€” base model: {base_model_path}")
        logger.info(f"              LoRA adapter: {lora_path}")
    else:
        load_path = model_path
        lora_path = None
        logger.info(f"ä½¿ç”¨ vLLM åŠ è½½å®Œæ•´æ¨¡å‹: {load_path}")

    gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
    tensor_parallel_size = vllm_config.get('tensor_parallel_size', 1)
    if gpu_count < tensor_parallel_size:
        logger.warning(f"å¯ç”¨GPUæ•°({gpu_count}) < è¯·æ±‚å¹¶è¡Œæ•°({tensor_parallel_size})ï¼Œå·²è‡ªåŠ¨è°ƒæ•´")
        tensor_parallel_size = max(1, gpu_count)

    engine = LLM(
        model=load_path,
        dtype=vllm_config.get('dtype', 'auto'),
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=vllm_config.get('gpu_memory_utilization', 0.9),
        max_model_len=vllm_config.get('max_model_len', 4096),
        trust_remote_code=True,
        enable_lora=is_lora,
        max_lora_rank=vllm_config.get('max_lora_rank', 64),
    )
    tokenizer = engine.get_tokenizer()

    lora_request = None
    if is_lora:
        lora_request = LoRARequest("sft_adapter", 1, lora_path)
        logger.info("âœ“ vLLM å¼•æ“ + LoRA é€‚é…å™¨åŠ è½½å®Œæˆ")
    else:
        logger.info("âœ“ vLLM æ¨¡å‹åŠ è½½å®Œæˆ")

    return engine, tokenizer, lora_request


def _build_prompt(tokenizer, system_prompt: str, question: str) -> str:
    """æ„å»ºå¸¦ chat template çš„ prompt å­—ç¬¦ä¸²"""
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question},
    ]
    if hasattr(tokenizer, "apply_chat_template"):
        return tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
    return (
        f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
        f"<|im_start|>user\n{question}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )


def generate_responses(
    model,
    tokenizer,
    question: str,
    system_prompt: str,
    gen_config: Dict,
    device: str
) -> List[Tuple[str, Dict]]:
    """ä½¿ç”¨ HuggingFace Transformers é€ç­–ç•¥ç”Ÿæˆå€™é€‰å›ç­”"""
    strategies = gen_config['strategies']
    responses = []

    text = _build_prompt(tokenizer, system_prompt, question)
    inputs = tokenizer(text, return_tensors="pt").to(device)

    for strategy in strategies:
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=gen_config['max_new_tokens'],
                temperature=strategy['temperature'],
                top_p=strategy['top_p'],
                do_sample=strategy['do_sample'],
                repetition_penalty=gen_config.get('repetition_penalty', 1.1),
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
        response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
        responses.append((response, strategy))

    return responses


def batch_generate_responses_vllm(
    engine,
    tokenizer,
    questions: List[str],
    system_prompt: str,
    gen_config: Dict,
    lora_request=None,
) -> List[List[Tuple[str, Dict]]]:
    """
    ä½¿ç”¨ vLLM æ‰¹é‡ç”Ÿæˆæ‰€æœ‰é—®é¢˜çš„å€™é€‰å›ç­”ã€‚

    æŒ‰ç­–ç•¥åˆ†ç»„æäº¤ï¼ˆK æ¬¡æ‰¹é‡è¯·æ±‚ï¼Œæ¯æ¬¡ N æ¡ï¼‰ï¼Œä¿è¯åŒæ‰¹é‡‡æ ·å‚æ•°ä¸€è‡´ï¼Œ
    å……åˆ†åˆ©ç”¨ vLLM çš„è¿ç»­æ‰¹å¤„ç†å’Œ PagedAttention èƒ½åŠ›ã€‚

    Args:
        engine: vLLM LLM å¼•æ“å®ä¾‹
        tokenizer: å¯¹åº”çš„ tokenizer
        questions: é—®é¢˜åˆ—è¡¨ï¼ˆé•¿åº¦ Nï¼‰
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        gen_config: generation é…ç½®å­—å…¸
        lora_request: LoRARequest å®ä¾‹ï¼ˆLoRA æ¨¡å‹æ—¶ä¼ å…¥ï¼Œå¦åˆ™ä¸º Noneï¼‰

    Returns:
        é•¿åº¦ä¸º N çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯è¯¥é—®é¢˜çš„å€™é€‰åˆ—è¡¨ [(response, strategy), ...]
    """
    strategies = gen_config['strategies']
    max_new_tokens = gen_config['max_new_tokens']
    repetition_penalty = gen_config.get('repetition_penalty', 1.1)

    # æ¯ä¸ªé—®é¢˜æ„å»ºä¸€æ¬¡ promptï¼ˆæ‰€æœ‰ç­–ç•¥å…±ç”¨åŒä¸€ prompt æ–‡æœ¬ï¼‰
    prompts = [_build_prompt(tokenizer, system_prompt, q) for q in questions]

    logger.info(
        f"vLLM æ‰¹é‡ç”Ÿæˆ: {len(questions)} ä¸ªé—®é¢˜ Ã— {len(strategies)} ä¸ªç­–ç•¥ "
        f"= {len(questions) * len(strategies)} æ¡è¯·æ±‚"
        + (" [LoRA æ¨¡å¼]" if lora_request else "")
    )

    results: List[List[Tuple[str, Dict]]] = [[] for _ in questions]

    for strategy in strategies:
        do_sample = strategy.get('do_sample', True)
        sp = SamplingParams(
            max_tokens=max_new_tokens,
            temperature=strategy['temperature'] if do_sample else 0.0,
            top_p=strategy['top_p'] if do_sample else 1.0,
            repetition_penalty=repetition_penalty,
            stop=["<|im_end|>"],
        )
        # æ‰¹é‡ç”Ÿæˆï¼ˆå¯é€‰ä¼ å…¥ lora_requestï¼‰
        generate_kwargs = {"lora_request": lora_request} if lora_request else {}
        outputs = engine.generate(prompts, sp, **generate_kwargs)
        for q_idx, output in enumerate(outputs):
            response = output.outputs[0].text.strip()
            results[q_idx].append((response, strategy))

    return results


def evaluate_responses(
    judge_model: JudgeModel,
    question: str,
    responses: List[Tuple[str, Dict]]
) -> List[ResponseCandidate]:
    """è¯„ä¼°å€™é€‰å›ç­”"""
    candidates = []
    
    for response, strategy in responses:
        try:
            scores = judge_model.evaluate_response(question, response)
            
            # è®¡ç®—ç»¼åˆå¾—åˆ† (qualityä¸ºä¸»ï¼Œå‡å»å¹»è§‰å’Œè¶Šæƒçš„æƒ©ç½š)
            overall_score = (
                scores['quality_score'] * 0.4 +
                scores['readability_score'] * 0.2 -
                scores['hallucination_score'] * 0.2 -
                scores['overreach_score'] * 0.2
            )
            
            candidate = ResponseCandidate(
                response=response,
                score=overall_score,
                hallucination_score=scores['hallucination_score'],
                overreach_score=scores['overreach_score'],
                quality_score=scores['quality_score'],
                readability_score=scores['readability_score'],
                details={
                    'strategy': strategy['name'],
                    'overall_comment': scores.get('overall_comment', ''),
                    'specific_issues': scores.get('specific_issues', [])
                }
            )
            candidates.append(candidate)
        except Exception as e:
            logger.warning(f"è¯„ä¼°å¤±è´¥: {e}")
            continue
    
    return candidates


def select_dpo_pair(
    candidates: List[ResponseCandidate],
    selection_config: Dict
) -> Optional[Tuple[ResponseCandidate, ResponseCandidate]]:
    """
    é€‰æ‹©chosenå’Œrejectedå¯¹
    
    æ–°é€»è¾‘ï¼š
    1. chosen: å…ˆå–quality_score Top-kï¼Œå†ä»ä¸­æŒ‘é€‰å¹»è§‰å’Œè¶Šæƒå¾—åˆ†æœ€ä½çš„
    2. rejected: ä»å‰©ä½™å€™é€‰ä¸­é€‰æ‹©è´¨é‡å·®ä½†ä»å¯è¯»çš„ï¼Œä¸”æœ‰æ˜æ˜¾é—®é¢˜ï¼ˆå¹»è§‰/è¶Šæƒï¼‰çš„
    """
    if len(candidates) < 2:
        return None
    
    chosen_criteria = selection_config['chosen_criteria']
    rejected_criteria = selection_config['rejected_criteria']
    
    # ========== æ­¥éª¤1: é€‰æ‹©chosen ==========
    # 1.1 å…ˆæŒ‰quality_scoreæ’åºï¼Œå–Top-k
    top_k = chosen_criteria.get('top_k', 3)  # é»˜è®¤Top-3
    sorted_by_quality = sorted(candidates, key=lambda c: c.quality_score, reverse=True)
    top_k_candidates = sorted_by_quality[:min(top_k, len(sorted_by_quality))]
    
    logger.debug(f"Top-{top_k} quality scores: {[c.quality_score for c in top_k_candidates]}")
    
    # 1.2 ä»Top-kä¸­ï¼ŒæŒ‘é€‰å¹»è§‰å’Œè¶Šæƒå¾—åˆ†éƒ½ä½çš„
    chosen_candidates = [
        c for c in top_k_candidates
        if (c.quality_score >= chosen_criteria['min_quality_score'] and
            c.hallucination_score <= chosen_criteria['max_hallucination_score'] and
            c.overreach_score <= chosen_criteria['max_overreach_score'] and
            c.readability_score >= chosen_criteria['min_readability_score'])
    ]
    
    if not chosen_candidates:
        logger.debug("æ²¡æœ‰ç¬¦åˆchosenæ¡ä»¶çš„å€™é€‰")
        return None
    
    # 1.3 åœ¨ç¬¦åˆæ¡ä»¶çš„å€™é€‰ä¸­ï¼Œé€‰æ‹©å¹»è§‰+è¶Šæƒå¾—åˆ†ä¹‹å’Œæœ€ä½çš„
    def compute_safety_score(c: ResponseCandidate) -> float:
        """å®‰å…¨æ€§å¾—åˆ†ï¼šå¹»è§‰+è¶Šæƒï¼ˆè¶Šä½è¶Šå®‰å…¨ï¼‰"""
        return c.hallucination_score + c.overreach_score
    
    chosen = min(chosen_candidates, key=compute_safety_score)
    
    logger.debug(f"Chosen: quality={chosen.quality_score:.2f}, "
                f"hallucination={chosen.hallucination_score:.2f}, "
                f"overreach={chosen.overreach_score:.2f}")
    
    # ========== æ­¥éª¤2: é€‰æ‹©rejected ==========
    # 2.1 ä»å‰©ä½™å€™é€‰ä¸­ç­›é€‰
    rejected_candidates = [
        c for c in candidates
        if (c != chosen and
            c.readability_score >= rejected_criteria['min_readability_score'] and
            c.readability_score <= rejected_criteria['max_readability_score'])
    ]
    
    if not rejected_candidates:
        logger.debug("æ²¡æœ‰ç¬¦åˆrejectedæ¡ä»¶çš„å€™é€‰")
        return None
    
    # 2.2 è®¡ç®—"è´Ÿæ ·æœ¬å¾—åˆ†"ï¼ˆè¶Šé«˜è¶Šé€‚åˆä½œä¸ºrejectedï¼‰
    # ç†æƒ³çš„rejectedï¼šè´¨é‡æ˜æ˜¾æ›´å·® + æœ‰æ˜æ˜¾çš„å¹»è§‰æˆ–è¶Šæƒé—®é¢˜ + ä½†ä»ç„¶å¯è¯»
    weights = rejected_criteria['weights']
    
    def compute_negative_score(candidate: ResponseCandidate) -> float:
        """
        è´Ÿæ ·æœ¬å¾—åˆ†è®¡ç®—
        - è´¨é‡å·®è·ï¼šchosenæ¯”å®ƒå¥½å¤šå°‘
        - é—®é¢˜æ˜æ˜¾æ€§ï¼šå¹»è§‰æˆ–è¶Šæƒé—®é¢˜è¶Šä¸¥é‡è¶Šå¥½
        - å¯è¯»æ€§é€‚ä¸­ï¼šä¸è¦å¤ªå·®ä¹Ÿä¸è¦å¤ªå¥½
        """
        score = 0.0
        
        # è´¨é‡å·®è·ï¼ˆè¶Šå¤§è¶Šå¥½ï¼Œè¯´æ˜chosenæ˜æ˜¾æ›´å¥½ï¼‰
        quality_gap = chosen.quality_score - candidate.quality_score
        if quality_gap < rejected_criteria['min_quality_gap']:
            return -1000  # è´¨é‡å·®è·ä¸å¤Ÿï¼Œä¸é€‚åˆä½œä¸ºrejected
        score += weights['quality'] * quality_gap
        
        # å¹»è§‰é—®é¢˜ï¼ˆè¶Šä¸¥é‡è¶Šå¥½ä½œä¸ºè´Ÿæ ·æœ¬ï¼‰
        score += weights['hallucination'] * candidate.hallucination_score
        
        # è¶Šæƒé—®é¢˜ï¼ˆè¶Šä¸¥é‡è¶Šå¥½ä½œä¸ºè´Ÿæ ·æœ¬ï¼‰
        score += weights['overreach'] * candidate.overreach_score
        
        # å¯è¯»æ€§ï¼šé€‚ä¸­æœ€å¥½ï¼ˆ5-7åˆ†ï¼‰ï¼Œå¤ªå·®æˆ–å¤ªå¥½éƒ½ä¸ç†æƒ³
        # å¤ªå·®ï¼šæ¨¡å‹å­¦ä¸åˆ°ä»€ä¹ˆï¼Œå¤ªå¥½ï¼šå®¹æ˜“æ··æ·†
        readability_penalty = abs(candidate.readability_score - 6.0)
        score -= weights['readability'] * readability_penalty
        
        return score
    
    # 2.3 é€‰æ‹©è´Ÿæ ·æœ¬å¾—åˆ†æœ€é«˜çš„ä½œä¸ºrejected
    rejected_scores = [(c, compute_negative_score(c)) for c in rejected_candidates]
    valid_rejected = [(c, s) for c, s in rejected_scores if s > 0]
    
    if not valid_rejected:
        logger.debug("æ²¡æœ‰æœ‰æ•ˆçš„rejectedå€™é€‰ï¼ˆè´¨é‡å·®è·ä¸å¤Ÿï¼‰")
        return None
    
    rejected, rejected_score = max(valid_rejected, key=lambda x: x[1])
    
    logger.debug(f"Rejected: quality={rejected.quality_score:.2f}, "
                f"hallucination={rejected.hallucination_score:.2f}, "
                f"overreach={rejected.overreach_score:.2f}, "
                f"negative_score={rejected_score:.2f}")
    
    # ========== æ­¥éª¤3: æœ€ç»ˆéªŒè¯ ==========
    # 3.1 éªŒè¯è´¨é‡å·®è·
    min_diff = selection_config.get('min_score_difference', 1.5)
    quality_diff = chosen.quality_score - rejected.quality_score
    
    if quality_diff < min_diff:
        logger.debug(f"è´¨é‡å·®è·ä¸è¶³: {quality_diff:.2f} < {min_diff}")
        return None
    
    # 3.2 éªŒè¯chosenç¡®å®æ›´å®‰å…¨ï¼ˆå¹»è§‰+è¶Šæƒæ›´ä½ï¼‰
    chosen_safety = compute_safety_score(chosen)
    rejected_safety = compute_safety_score(rejected)
    
    if chosen_safety >= rejected_safety:
        logger.debug(f"Chosenä¸å¤Ÿå®‰å…¨: {chosen_safety:.2f} >= {rejected_safety:.2f}")
        # å…è®¸ä¸€å®šå®¹å¿åº¦ï¼ˆå¦‚æœè´¨é‡å·®è·å¾ˆå¤§ï¼‰
        if quality_diff < min_diff * 2:
            return None
    
    logger.debug(f"âœ“ é€‰æ‹©æˆåŠŸ: quality_gap={quality_diff:.2f}, "
                f"safety_gap={rejected_safety - chosen_safety:.2f}")
    
    return (chosen, rejected)


def construct_dpo_data(config_path: str):
    """æ„é€ DPOæ•°æ®"""
    # åŠ è½½é…ç½®
    logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    config = load_config(config_path)
    
    # æå–é…ç½®
    input_config = config['input_data']
    sft_model_config = config['sft_model']
    gen_config = config['generation']
    judge_config = config['judge_model']
    selection_config = config['selection_strategy']
    output_config = config['output']
    
    # è·¯å¾„å¤„ç†
    data_path = input_config['data_path']
    if not os.path.isabs(data_path):
        data_path = os.path.join(project_root, data_path)
    
    output_path = output_config['output_path']
    if not os.path.isabs(output_path):
        output_path = os.path.join(project_root, output_path)
    
    report_path = output_config.get('report_path')
    if report_path and not os.path.isabs(report_path):
        report_path = os.path.join(project_root, report_path)
    
    logger.info("\n" + "=" * 70)
    logger.info("DPOè´Ÿæ ·æœ¬æ„é€ é…ç½®")
    logger.info("=" * 70)
    logger.info(f"è¾“å…¥æ•°æ®: {data_path}")
    logger.info(f"æ ·æœ¬æ•°é‡: {input_config.get('num_samples') or 'å…¨éƒ¨'}")
    logger.info(f"æ¯ä¸ªé—®é¢˜ç”Ÿæˆ: {len(gen_config['strategies'])} ä¸ªå€™é€‰å›ç­”")
    logger.info(f"è¾“å‡ºè·¯å¾„: {output_path}")
    logger.info("=" * 70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # 1. åŠ è½½è¾“å…¥æ•°æ®
    logger.info(f"\nğŸ“‚ åŠ è½½è¾“å…¥æ•°æ®...")
    with open(data_path, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
    logger.info(f"æ€»æ ·æœ¬æ•°: {len(input_data)}")
    
    # é‡‡æ ·
    num_samples = input_config.get('num_samples')
    random_seed = input_config.get('random_seed', 42)
    random.seed(random_seed)
    
    if num_samples and num_samples < len(input_data):
        input_samples = random.sample(input_data, num_samples)
        logger.info(f"éšæœºæŠ½æ · {num_samples} ä¸ªæ ·æœ¬")
    else:
        input_samples = input_data
    
    # 2. åŠ è½½SFTæ¨¡å‹ï¼ˆvLLM æˆ– HuggingFaceï¼‰
    vllm_config = config.get('vllm', {})
    use_vllm = vllm_config.get('enabled', False)
    device = sft_model_config['device']

    if use_vllm:
        logger.info("\nğŸš€ ä½¿ç”¨ vLLM æ¨ç†åç«¯åŠ è½½æ¨¡å‹...")
        engine, tokenizer, lora_request = load_sft_model_vllm(
            sft_model_config, vllm_config, project_root
        )
        model = None  # vLLM è·¯å¾„ä¸ä½¿ç”¨ HuggingFace model å¯¹è±¡
    else:
        logger.info("\nğŸ“¥ ä½¿ç”¨ HuggingFace Transformers åŠ è½½æ¨¡å‹...")
        model, tokenizer = load_sft_model(sft_model_config, project_root)
        engine = None
        lora_request = None

    # 3. åˆå§‹åŒ–è¯„å®¡æ¨¡å‹
    logger.info(f"\nğŸ” åˆå§‹åŒ–è¯„å®¡æ¨¡å‹...")
    api_key = os.environ.get('DEEPSEEK_API_KEY') or judge_config.get('api_key', '')
    if not api_key:
        logger.error("âŒ é”™è¯¯: æœªè®¾ç½®APIå¯†é’¥")
        logger.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY=your_key")
        return
    
    judge_model = JudgeModel(
        api_key=api_key,
        base_url=judge_config['base_url'],
        model=judge_config.get('model', 'deepseek-chat')
    )
    logger.info("âœ“ è¯„å®¡æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # 4. æ„é€ DPOæ ·æœ¬
    logger.info(f"\nğŸ”§ å¼€å§‹æ„é€ DPOæ ·æœ¬...")
    system_prompt = config.get('system_prompt', 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚')

    dpo_samples = []
    stats = GenerationStats()
    stats.total_samples = len(input_samples)

    all_candidates_data = []  # ä¿å­˜æ‰€æœ‰å€™é€‰å›ç­”ï¼ˆç”¨äºåˆ†æï¼‰

    # æå–æœ‰æ•ˆé—®é¢˜åˆ—è¡¨ï¼ˆè¿‡æ»¤ç©ºé—®é¢˜ï¼‰
    valid_samples = []
    for idx, sample in enumerate(input_samples):
        question = sample.get('question') or sample.get('query') or sample.get('instruction') or ""
        if question:
            valid_samples.append((idx, sample, question))
        else:
            stats.skipped_samples += 1

    # â”€â”€ vLLM è·¯å¾„ï¼šä¸€æ¬¡æ€§æ‰¹é‡ç”Ÿæˆæ‰€æœ‰é—®é¢˜çš„æ‰€æœ‰å€™é€‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if use_vllm:
        logger.info(f"vLLM æ‰¹é‡ç”Ÿæˆæ¨¡å¼ï¼šå…± {len(valid_samples)} ä¸ªé—®é¢˜")
        all_questions = [q for _, _, q in valid_samples]
        all_responses_batch = batch_generate_responses_vllm(
            engine, tokenizer, all_questions, system_prompt, gen_config,
            lora_request=lora_request,
        )
        # all_responses_batch[i] = [(response, strategy), ...] for valid_samples[i]

        for batch_idx, (orig_idx, sample, question) in enumerate(
            tqdm(valid_samples, desc="è¯„ä¼°ä¸é€‰æ‹©DPOæ ·æœ¬")
        ):
            try:
                responses = all_responses_batch[batch_idx]
                candidates = evaluate_responses(judge_model, question, responses)

                if not candidates:
                    stats.skipped_samples += 1
                    continue

                stats.generated_samples += 1
                stats.avg_candidates_per_sample += len(candidates)

                if output_config.get('save_all_candidates', True):
                    all_candidates_data.append({
                        'question': question,
                        'candidates': [asdict(c) for c in candidates],
                        'sample_id': sample.get('id', f'sample_{orig_idx}')
                    })

                pair = select_dpo_pair(candidates, selection_config)
                if pair is None:
                    stats.invalid_pairs += 1
                    continue

                chosen, rejected = pair
                stats.valid_pairs += 1
                stats.avg_score_difference += (chosen.quality_score - rejected.quality_score)

                dpo_samples.append(DPOSample(
                    prompt=question,
                    chosen=chosen.response,
                    rejected=rejected.response,
                    chosen_score=chosen.score,
                    rejected_score=rejected.score,
                    metadata={
                        'source_id': sample.get('id', f'sample_{orig_idx}'),
                        'chosen_strategy': chosen.details.get('strategy'),
                        'rejected_strategy': rejected.details.get('strategy'),
                        'score_difference': chosen.quality_score - rejected.quality_score,
                        'num_candidates': len(candidates),
                        'chosen_scores': {
                            'overall': chosen.score,
                            'hallucination': chosen.hallucination_score,
                            'overreach': chosen.overreach_score,
                            'quality': chosen.quality_score,
                            'readability': chosen.readability_score
                        },
                        'rejected_scores': {
                            'overall': rejected.score,
                            'hallucination': rejected.hallucination_score,
                            'overreach': rejected.overreach_score,
                            'quality': rejected.quality_score,
                            'readability': rejected.readability_score
                        }
                    }
                ))

            except Exception as e:
                logger.warning(f"å¤„ç†æ ·æœ¬ {orig_idx} å¤±è´¥: {e}")
                stats.skipped_samples += 1
                continue

    # â”€â”€ HuggingFace è·¯å¾„ï¼šé€æ ·æœ¬é¡ºåºç”Ÿæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    else:
        for orig_idx, sample, question in tqdm(valid_samples, desc="æ„é€ DPOæ ·æœ¬"):
            try:
                responses = generate_responses(
                    model, tokenizer, question, system_prompt, gen_config, device
                )
                candidates = evaluate_responses(judge_model, question, responses)

                if not candidates:
                    stats.skipped_samples += 1
                    continue

                stats.generated_samples += 1
                stats.avg_candidates_per_sample += len(candidates)

                if output_config.get('save_all_candidates', True):
                    all_candidates_data.append({
                        'question': question,
                        'candidates': [asdict(c) for c in candidates],
                        'sample_id': sample.get('id', f'sample_{orig_idx}')
                    })

                pair = select_dpo_pair(candidates, selection_config)
                if pair is None:
                    stats.invalid_pairs += 1
                    continue

                chosen, rejected = pair
                stats.valid_pairs += 1
                stats.avg_score_difference += (chosen.quality_score - rejected.quality_score)

                dpo_samples.append(DPOSample(
                    prompt=question,
                    chosen=chosen.response,
                    rejected=rejected.response,
                    chosen_score=chosen.score,
                    rejected_score=rejected.score,
                    metadata={
                        'source_id': sample.get('id', f'sample_{orig_idx}'),
                        'chosen_strategy': chosen.details.get('strategy'),
                        'rejected_strategy': rejected.details.get('strategy'),
                        'score_difference': chosen.quality_score - rejected.quality_score,
                        'num_candidates': len(candidates),
                        'chosen_scores': {
                            'overall': chosen.score,
                            'hallucination': chosen.hallucination_score,
                            'overreach': chosen.overreach_score,
                            'quality': chosen.quality_score,
                            'readability': chosen.readability_score
                        },
                        'rejected_scores': {
                            'overall': rejected.score,
                            'hallucination': rejected.hallucination_score,
                            'overreach': rejected.overreach_score,
                            'quality': rejected.quality_score,
                            'readability': rejected.readability_score
                        }
                    }
                ))

            except Exception as e:
                logger.warning(f"å¤„ç†æ ·æœ¬ {orig_idx} å¤±è´¥: {e}")
                stats.skipped_samples += 1
                continue
    
    # è®¡ç®—å¹³å‡å€¼
    if stats.generated_samples > 0:
        stats.avg_candidates_per_sample /= stats.generated_samples
    if stats.valid_pairs > 0:
        stats.avg_score_difference /= stats.valid_pairs
    
    # 5. ä¿å­˜ç»“æœ
    logger.info(f"\nğŸ’¾ ä¿å­˜DPOæ•°æ®...")
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    dpo_data = [asdict(sample) for sample in dpo_samples]
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(dpo_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ“ DPOæ•°æ®å·²ä¿å­˜: {output_path}")
    logger.info(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {len(dpo_data)}")
    
    # ä¿å­˜æ‰€æœ‰å€™é€‰ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if output_config.get('save_all_candidates', True) and all_candidates_data:
        candidates_path = output_path.replace('.json', '_all_candidates.json')
        with open(candidates_path, 'w', encoding='utf-8') as f:
            json.dump(all_candidates_data, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ“ æ‰€æœ‰å€™é€‰å·²ä¿å­˜: {candidates_path}")
    
    # 6. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    if output_config.get('save_report', True) and report_path:
        logger.info(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'config': config,
            'statistics': asdict(stats),
            'success_rate': stats.valid_pairs / stats.total_samples if stats.total_samples > 0 else 0,
            'sample_quality': {
                'avg_score_difference': stats.avg_score_difference,
                'avg_candidates_per_sample': stats.avg_candidates_per_sample
            }
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ“ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # 7. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"\n" + "=" * 70)
    logger.info("DPOæ ·æœ¬æ„é€ å®Œæˆ")
    logger.info("=" * 70)
    logger.info(f"æ€»æ ·æœ¬æ•°: {stats.total_samples}")
    logger.info(f"æˆåŠŸç”Ÿæˆ: {stats.generated_samples}")
    logger.info(f"æœ‰æ•ˆDPOå¯¹: {stats.valid_pairs}")
    logger.info(f"æ— æ•ˆDPOå¯¹: {stats.invalid_pairs}")
    logger.info(f"è·³è¿‡æ ·æœ¬: {stats.skipped_samples}")
    logger.info(f"æˆåŠŸç‡: {stats.valid_pairs / stats.total_samples * 100:.1f}%")
    logger.info(f"å¹³å‡å€™é€‰æ•°: {stats.avg_candidates_per_sample:.1f}")
    logger.info(f"å¹³å‡åˆ†å·®: {stats.avg_score_difference:.2f}")
    logger.info("=" * 70)
    
    # è´¨é‡æ£€æŸ¥
    quality_control = config.get('quality_control', {})
    min_valid_pairs = quality_control.get('min_valid_pairs', 10)
    
    if stats.valid_pairs < min_valid_pairs:
        logger.warning(f"âš ï¸ è­¦å‘Š: æœ‰æ•ˆæ ·æœ¬æ•° ({stats.valid_pairs}) å°‘äºæœ€å°è¦æ±‚ ({min_valid_pairs})")
        logger.warning("å»ºè®®è°ƒæ•´é€‰æ‹©ç­–ç•¥æˆ–å¢åŠ è¾“å…¥æ ·æœ¬æ•°")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ„é€ DPOè®­ç»ƒæ•°æ®")
    parser.add_argument(
        "--config",
        type=str,
        default="config/dpo_construction_config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = args.config
    if not os.path.isabs(config_path):
        config_path = os.path.join(project_root, config_path)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(config_path):
        logger.error(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    # è¿è¡Œæ„é€ 
    try:
        construct_dpo_data(config_path)
    except Exception as e:
        logger.error(f"\nâŒ æ„é€ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
# python src/training/scripts/run_dpo_construction.py --config config/dpo_construction_config.yaml
