#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DPOè´Ÿæ ·æœ¬æ„é€ è„šæœ¬
ä»SFTæ•°æ®ç”ŸæˆDPOè®­ç»ƒæ‰€éœ€çš„chosen/rejectedå¯¹
"""

import os
import sys
import json
import yaml
import torch
import random
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from tqdm import tqdm
from dataclasses import dataclass, asdict
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
script_dir = Path(__file__).resolve().parent
training_dir = script_dir.parent
src_dir = training_dir.parent
project_root = src_dir.parent
sys.path.insert(0, str(project_root))

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from src.training.dataset.dpo_negative_constructor import (
    ResponseCandidate, 
    DPOSample, 
    JudgeModel
)

import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class GenerationStats:
    """ç”Ÿæˆç»Ÿè®¡"""
    total_samples: int = 0
    generated_samples: int = 0
    valid_pairs: int = 0
    invalid_pairs: int = 0
    skipped_samples: int = 0
    avg_candidates_per_sample: float = 0.0
    avg_score_difference: float = 0.0


def load_config(config_path: str) -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_sft_model(model_config: Dict, project_root: Path):
    """åŠ è½½SFTæ¨¡å‹"""
    model_path = model_config['model_path']
    if not os.path.isabs(model_path):
        model_path = os.path.join(project_root, model_path)
    
    is_lora = model_config.get('is_lora', False)
    device = model_config['device']
    
    if is_lora:
        base_model_path = model_config.get('base_model_path')
        if not os.path.isabs(base_model_path):
            base_model_path = os.path.join(project_root, base_model_path)
        
        logger.info(f"åŠ è½½LoRAæ¨¡å‹...")
        logger.info(f"  åŸºç¡€æ¨¡å‹: {base_model_path}")
        logger.info(f"  LoRAé€‚é…å™¨: {model_path}")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            padding_side="left"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        # åŠ è½½LoRAé€‚é…å™¨
        model = PeftModel.from_pretrained(base_model, model_path)
        model.eval()
        
        logger.info("âœ“ LoRAæ¨¡å‹åŠ è½½å®Œæˆ")
    else:
        logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="left"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        model.eval()
        
        logger.info("âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    return model, tokenizer


def generate_responses(
    model,
    tokenizer,
    question: str,
    system_prompt: str,
    gen_config: Dict,
    device: str
) -> List[Tuple[str, Dict]]:
    """ç”Ÿæˆå¤šä¸ªå€™é€‰å›ç­”"""
    strategies = gen_config['strategies']
    responses = []
    
    for strategy in strategies:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question}
        ]
        
        # æ„é€ è¾“å…¥
        text = f"<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = tokenizer(text, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=gen_config['max_new_tokens'],
                temperature=strategy['temperature'],
                top_p=strategy['top_p'],
                do_sample=strategy['do_sample'],
                repetition_penalty=gen_config.get('repetition_penalty', 1.1),
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        generated_ids = outputs[0][inputs['input_ids'].shape[1]:]
        response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
        
        responses.append((response, strategy))
    
    return responses


def evaluate_responses(
    judge_model: JudgeModel,
    question: str,
    responses: List[Tuple[str, Dict]]
) -> List[ResponseCandidate]:
    """è¯„ä¼°å€™é€‰å›ç­”"""
    candidates = []
    
    for response, strategy in responses:
        try:
            scores = judge_model.evaluate_response(question, response)
            
            # è®¡ç®—ç»¼åˆå¾—åˆ† (qualityä¸ºä¸»ï¼Œå‡å»å¹»è§‰å’Œè¶Šæƒçš„æƒ©ç½š)
            overall_score = (
                scores['quality_score'] * 0.4 +
                scores['readability_score'] * 0.2 -
                scores['hallucination_score'] * 0.2 -
                scores['overreach_score'] * 0.2
            )
            
            candidate = ResponseCandidate(
                response=response,
                score=overall_score,
                hallucination_score=scores['hallucination_score'],
                overreach_score=scores['overreach_score'],
                quality_score=scores['quality_score'],
                readability_score=scores['readability_score'],
                details={
                    'strategy': strategy['name'],
                    'overall_comment': scores.get('overall_comment', ''),
                    'specific_issues': scores.get('specific_issues', [])
                }
            )
            candidates.append(candidate)
        except Exception as e:
            logger.warning(f"è¯„ä¼°å¤±è´¥: {e}")
            continue
    
    return candidates


def select_dpo_pair(
    candidates: List[ResponseCandidate],
    selection_config: Dict
) -> Optional[Tuple[ResponseCandidate, ResponseCandidate]]:
    """
    é€‰æ‹©chosenå’Œrejectedå¯¹
    
    æ–°é€»è¾‘ï¼š
    1. chosen: å…ˆå–quality_score Top-kï¼Œå†ä»ä¸­æŒ‘é€‰å¹»è§‰å’Œè¶Šæƒå¾—åˆ†æœ€ä½çš„
    2. rejected: ä»å‰©ä½™å€™é€‰ä¸­é€‰æ‹©è´¨é‡å·®ä½†ä»å¯è¯»çš„ï¼Œä¸”æœ‰æ˜æ˜¾é—®é¢˜ï¼ˆå¹»è§‰/è¶Šæƒï¼‰çš„
    """
    if len(candidates) < 2:
        return None
    
    chosen_criteria = selection_config['chosen_criteria']
    rejected_criteria = selection_config['rejected_criteria']
    
    # ========== æ­¥éª¤1: é€‰æ‹©chosen ==========
    # 1.1 å…ˆæŒ‰quality_scoreæ’åºï¼Œå–Top-k
    top_k = chosen_criteria.get('top_k', 3)  # é»˜è®¤Top-3
    sorted_by_quality = sorted(candidates, key=lambda c: c.quality_score, reverse=True)
    top_k_candidates = sorted_by_quality[:min(top_k, len(sorted_by_quality))]
    
    logger.debug(f"Top-{top_k} quality scores: {[c.quality_score for c in top_k_candidates]}")
    
    # 1.2 ä»Top-kä¸­ï¼ŒæŒ‘é€‰å¹»è§‰å’Œè¶Šæƒå¾—åˆ†éƒ½ä½çš„
    chosen_candidates = [
        c for c in top_k_candidates
        if (c.quality_score >= chosen_criteria['min_quality_score'] and
            c.hallucination_score <= chosen_criteria['max_hallucination_score'] and
            c.overreach_score <= chosen_criteria['max_overreach_score'] and
            c.readability_score >= chosen_criteria['min_readability_score'])
    ]
    
    if not chosen_candidates:
        logger.debug("æ²¡æœ‰ç¬¦åˆchosenæ¡ä»¶çš„å€™é€‰")
        return None
    
    # 1.3 åœ¨ç¬¦åˆæ¡ä»¶çš„å€™é€‰ä¸­ï¼Œé€‰æ‹©å¹»è§‰+è¶Šæƒå¾—åˆ†ä¹‹å’Œæœ€ä½çš„
    def compute_safety_score(c: ResponseCandidate) -> float:
        """å®‰å…¨æ€§å¾—åˆ†ï¼šå¹»è§‰+è¶Šæƒï¼ˆè¶Šä½è¶Šå®‰å…¨ï¼‰"""
        return c.hallucination_score + c.overreach_score
    
    chosen = min(chosen_candidates, key=compute_safety_score)
    
    logger.debug(f"Chosen: quality={chosen.quality_score:.2f}, "
                f"hallucination={chosen.hallucination_score:.2f}, "
                f"overreach={chosen.overreach_score:.2f}")
    
    # ========== æ­¥éª¤2: é€‰æ‹©rejected ==========
    # 2.1 ä»å‰©ä½™å€™é€‰ä¸­ç­›é€‰
    rejected_candidates = [
        c for c in candidates
        if (c != chosen and
            c.readability_score >= rejected_criteria['min_readability_score'] and
            c.readability_score <= rejected_criteria['max_readability_score'])
    ]
    
    if not rejected_candidates:
        logger.debug("æ²¡æœ‰ç¬¦åˆrejectedæ¡ä»¶çš„å€™é€‰")
        return None
    
    # 2.2 è®¡ç®—"è´Ÿæ ·æœ¬å¾—åˆ†"ï¼ˆè¶Šé«˜è¶Šé€‚åˆä½œä¸ºrejectedï¼‰
    # ç†æƒ³çš„rejectedï¼šè´¨é‡æ˜æ˜¾æ›´å·® + æœ‰æ˜æ˜¾çš„å¹»è§‰æˆ–è¶Šæƒé—®é¢˜ + ä½†ä»ç„¶å¯è¯»
    weights = rejected_criteria['weights']
    
    def compute_negative_score(candidate: ResponseCandidate) -> float:
        """
        è´Ÿæ ·æœ¬å¾—åˆ†è®¡ç®—
        - è´¨é‡å·®è·ï¼šchosenæ¯”å®ƒå¥½å¤šå°‘
        - é—®é¢˜æ˜æ˜¾æ€§ï¼šå¹»è§‰æˆ–è¶Šæƒé—®é¢˜è¶Šä¸¥é‡è¶Šå¥½
        - å¯è¯»æ€§é€‚ä¸­ï¼šä¸è¦å¤ªå·®ä¹Ÿä¸è¦å¤ªå¥½
        """
        score = 0.0
        
        # è´¨é‡å·®è·ï¼ˆè¶Šå¤§è¶Šå¥½ï¼Œè¯´æ˜chosenæ˜æ˜¾æ›´å¥½ï¼‰
        quality_gap = chosen.quality_score - candidate.quality_score
        if quality_gap < rejected_criteria['min_quality_gap']:
            return -1000  # è´¨é‡å·®è·ä¸å¤Ÿï¼Œä¸é€‚åˆä½œä¸ºrejected
        score += weights['quality'] * quality_gap
        
        # å¹»è§‰é—®é¢˜ï¼ˆè¶Šä¸¥é‡è¶Šå¥½ä½œä¸ºè´Ÿæ ·æœ¬ï¼‰
        score += weights['hallucination'] * candidate.hallucination_score
        
        # è¶Šæƒé—®é¢˜ï¼ˆè¶Šä¸¥é‡è¶Šå¥½ä½œä¸ºè´Ÿæ ·æœ¬ï¼‰
        score += weights['overreach'] * candidate.overreach_score
        
        # å¯è¯»æ€§ï¼šé€‚ä¸­æœ€å¥½ï¼ˆ5-7åˆ†ï¼‰ï¼Œå¤ªå·®æˆ–å¤ªå¥½éƒ½ä¸ç†æƒ³
        # å¤ªå·®ï¼šæ¨¡å‹å­¦ä¸åˆ°ä»€ä¹ˆï¼Œå¤ªå¥½ï¼šå®¹æ˜“æ··æ·†
        readability_penalty = abs(candidate.readability_score - 6.0)
        score -= weights['readability'] * readability_penalty
        
        return score
    
    # 2.3 é€‰æ‹©è´Ÿæ ·æœ¬å¾—åˆ†æœ€é«˜çš„ä½œä¸ºrejected
    rejected_scores = [(c, compute_negative_score(c)) for c in rejected_candidates]
    valid_rejected = [(c, s) for c, s in rejected_scores if s > 0]
    
    if not valid_rejected:
        logger.debug("æ²¡æœ‰æœ‰æ•ˆçš„rejectedå€™é€‰ï¼ˆè´¨é‡å·®è·ä¸å¤Ÿï¼‰")
        return None
    
    rejected, rejected_score = max(valid_rejected, key=lambda x: x[1])
    
    logger.debug(f"Rejected: quality={rejected.quality_score:.2f}, "
                f"hallucination={rejected.hallucination_score:.2f}, "
                f"overreach={rejected.overreach_score:.2f}, "
                f"negative_score={rejected_score:.2f}")
    
    # ========== æ­¥éª¤3: æœ€ç»ˆéªŒè¯ ==========
    # 3.1 éªŒè¯è´¨é‡å·®è·
    min_diff = selection_config.get('min_score_difference', 1.5)
    quality_diff = chosen.quality_score - rejected.quality_score
    
    if quality_diff < min_diff:
        logger.debug(f"è´¨é‡å·®è·ä¸è¶³: {quality_diff:.2f} < {min_diff}")
        return None
    
    # 3.2 éªŒè¯chosenç¡®å®æ›´å®‰å…¨ï¼ˆå¹»è§‰+è¶Šæƒæ›´ä½ï¼‰
    chosen_safety = compute_safety_score(chosen)
    rejected_safety = compute_safety_score(rejected)
    
    if chosen_safety >= rejected_safety:
        logger.debug(f"Chosenä¸å¤Ÿå®‰å…¨: {chosen_safety:.2f} >= {rejected_safety:.2f}")
        # å…è®¸ä¸€å®šå®¹å¿åº¦ï¼ˆå¦‚æœè´¨é‡å·®è·å¾ˆå¤§ï¼‰
        if quality_diff < min_diff * 2:
            return None
    
    logger.debug(f"âœ“ é€‰æ‹©æˆåŠŸ: quality_gap={quality_diff:.2f}, "
                f"safety_gap={rejected_safety - chosen_safety:.2f}")
    
    return (chosen, rejected)


def construct_dpo_data(config_path: str):
    """æ„é€ DPOæ•°æ®"""
    # åŠ è½½é…ç½®
    logger.info(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
    config = load_config(config_path)
    
    # æå–é…ç½®
    input_config = config['input_data']
    sft_model_config = config['sft_model']
    gen_config = config['generation']
    judge_config = config['judge_model']
    selection_config = config['selection_strategy']
    output_config = config['output']
    
    # è·¯å¾„å¤„ç†
    data_path = input_config['data_path']
    if not os.path.isabs(data_path):
        data_path = os.path.join(project_root, data_path)
    
    output_path = output_config['output_path']
    if not os.path.isabs(output_path):
        output_path = os.path.join(project_root, output_path)
    
    report_path = output_config.get('report_path')
    if report_path and not os.path.isabs(report_path):
        report_path = os.path.join(project_root, report_path)
    
    logger.info("\n" + "=" * 70)
    logger.info("DPOè´Ÿæ ·æœ¬æ„é€ é…ç½®")
    logger.info("=" * 70)
    logger.info(f"è¾“å…¥æ•°æ®: {data_path}")
    logger.info(f"æ ·æœ¬æ•°é‡: {input_config.get('num_samples') or 'å…¨éƒ¨'}")
    logger.info(f"æ¯ä¸ªé—®é¢˜ç”Ÿæˆ: {len(gen_config['strategies'])} ä¸ªå€™é€‰å›ç­”")
    logger.info(f"è¾“å‡ºè·¯å¾„: {output_path}")
    logger.info("=" * 70)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    # 1. åŠ è½½è¾“å…¥æ•°æ®
    logger.info(f"\nğŸ“‚ åŠ è½½è¾“å…¥æ•°æ®...")
    with open(data_path, 'r', encoding='utf-8') as f:
        input_data = json.load(f)
    logger.info(f"æ€»æ ·æœ¬æ•°: {len(input_data)}")
    
    # é‡‡æ ·
    num_samples = input_config.get('num_samples')
    random_seed = input_config.get('random_seed', 42)
    random.seed(random_seed)
    
    if num_samples and num_samples < len(input_data):
        input_samples = random.sample(input_data, num_samples)
        logger.info(f"éšæœºæŠ½æ · {num_samples} ä¸ªæ ·æœ¬")
    else:
        input_samples = input_data
    
    # 2. åŠ è½½SFTæ¨¡å‹
    device = sft_model_config['device']
    model, tokenizer = load_sft_model(sft_model_config, project_root)
    
    # 3. åˆå§‹åŒ–è¯„å®¡æ¨¡å‹
    logger.info(f"\nğŸ” åˆå§‹åŒ–è¯„å®¡æ¨¡å‹...")
    api_key = os.environ.get('DEEPSEEK_API_KEY') or judge_config.get('api_key', '')
    if not api_key:
        logger.error("âŒ é”™è¯¯: æœªè®¾ç½®APIå¯†é’¥")
        logger.error("è¯·è®¾ç½®ç¯å¢ƒå˜é‡: export DEEPSEEK_API_KEY=your_key")
        return
    
    judge_model = JudgeModel(
        api_key=api_key,
        base_url=judge_config['base_url'],
        model=judge_config.get('model', 'deepseek-chat')
    )
    logger.info("âœ“ è¯„å®¡æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    # 4. æ„é€ DPOæ ·æœ¬
    logger.info(f"\nğŸ”§ å¼€å§‹æ„é€ DPOæ ·æœ¬...")
    system_prompt = config.get('system_prompt', 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚')
    
    dpo_samples = []
    stats = GenerationStats()
    stats.total_samples = len(input_samples)
    
    all_candidates_data = []  # ä¿å­˜æ‰€æœ‰å€™é€‰å›ç­”ï¼ˆç”¨äºåˆ†æï¼‰
    
    for idx, sample in enumerate(tqdm(input_samples, desc="æ„é€ DPOæ ·æœ¬")):
        question = sample.get('question') or sample.get('query') or sample.get('instruction') or ""
        
        if not question:
            stats.skipped_samples += 1
            continue
        
        try:
            # ç”Ÿæˆå€™é€‰å›ç­”
            responses = generate_responses(
                model, tokenizer, question, system_prompt, gen_config, device
            )
            
            # è¯„ä¼°å€™é€‰å›ç­”
            candidates = evaluate_responses(judge_model, question, responses)
            
            if not candidates:
                stats.skipped_samples += 1
                continue
            
            stats.generated_samples += 1
            stats.avg_candidates_per_sample += len(candidates)
            
            # ä¿å­˜æ‰€æœ‰å€™é€‰ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if output_config.get('save_all_candidates', True):
                all_candidates_data.append({
                    'question': question,
                    'candidates': [asdict(c) for c in candidates],
                    'sample_id': sample.get('id', f'sample_{idx}')
                })
            
            # é€‰æ‹©chosenå’Œrejectedå¯¹
            pair = select_dpo_pair(candidates, selection_config)
            
            if pair is None:
                stats.invalid_pairs += 1
                continue
            
            chosen, rejected = pair
            stats.valid_pairs += 1
            stats.avg_score_difference += (chosen.quality_score - rejected.quality_score)
            
            # åˆ›å»ºDPOæ ·æœ¬
            dpo_sample = DPOSample(
                prompt=question,
                chosen=chosen.response,
                rejected=rejected.response,
                chosen_score=chosen.score,  # ä½¿ç”¨ç»¼åˆå¾—åˆ†
                rejected_score=rejected.score,  # ä½¿ç”¨ç»¼åˆå¾—åˆ†
                metadata={
                    'source_id': sample.get('id', f'sample_{idx}'),
                    'chosen_strategy': chosen.details.get('strategy'),
                    'rejected_strategy': rejected.details.get('strategy'),
                    'score_difference': chosen.quality_score - rejected.quality_score,
                    'num_candidates': len(candidates),
                    # è¯¦ç»†åˆ†æ•°å­˜åœ¨metadataä¸­
                    'chosen_scores': {
                        'overall': chosen.score,
                        'hallucination': chosen.hallucination_score,
                        'overreach': chosen.overreach_score,
                        'quality': chosen.quality_score,
                        'readability': chosen.readability_score
                    },
                    'rejected_scores': {
                        'overall': rejected.score,
                        'hallucination': rejected.hallucination_score,
                        'overreach': rejected.overreach_score,
                        'quality': rejected.quality_score,
                        'readability': rejected.readability_score
                    }
                }
            )
            
            dpo_samples.append(dpo_sample)
            
        except Exception as e:
            logger.warning(f"å¤„ç†æ ·æœ¬ {idx} å¤±è´¥: {e}")
            stats.skipped_samples += 1
            continue
    
    # è®¡ç®—å¹³å‡å€¼
    if stats.generated_samples > 0:
        stats.avg_candidates_per_sample /= stats.generated_samples
    if stats.valid_pairs > 0:
        stats.avg_score_difference /= stats.valid_pairs
    
    # 5. ä¿å­˜ç»“æœ
    logger.info(f"\nğŸ’¾ ä¿å­˜DPOæ•°æ®...")
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    dpo_data = [asdict(sample) for sample in dpo_samples]
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(dpo_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ“ DPOæ•°æ®å·²ä¿å­˜: {output_path}")
    logger.info(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {len(dpo_data)}")
    
    # ä¿å­˜æ‰€æœ‰å€™é€‰ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if output_config.get('save_all_candidates', True) and all_candidates_data:
        candidates_path = output_path.replace('.json', '_all_candidates.json')
        with open(candidates_path, 'w', encoding='utf-8') as f:
            json.dump(all_candidates_data, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ“ æ‰€æœ‰å€™é€‰å·²ä¿å­˜: {candidates_path}")
    
    # 6. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    if output_config.get('save_report', True) and report_path:
        logger.info(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'config': config,
            'statistics': asdict(stats),
            'success_rate': stats.valid_pairs / stats.total_samples if stats.total_samples > 0 else 0,
            'sample_quality': {
                'avg_score_difference': stats.avg_score_difference,
                'avg_candidates_per_sample': stats.avg_candidates_per_sample
            }
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ“ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    # 7. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"\n" + "=" * 70)
    logger.info("DPOæ ·æœ¬æ„é€ å®Œæˆ")
    logger.info("=" * 70)
    logger.info(f"æ€»æ ·æœ¬æ•°: {stats.total_samples}")
    logger.info(f"æˆåŠŸç”Ÿæˆ: {stats.generated_samples}")
    logger.info(f"æœ‰æ•ˆDPOå¯¹: {stats.valid_pairs}")
    logger.info(f"æ— æ•ˆDPOå¯¹: {stats.invalid_pairs}")
    logger.info(f"è·³è¿‡æ ·æœ¬: {stats.skipped_samples}")
    logger.info(f"æˆåŠŸç‡: {stats.valid_pairs / stats.total_samples * 100:.1f}%")
    logger.info(f"å¹³å‡å€™é€‰æ•°: {stats.avg_candidates_per_sample:.1f}")
    logger.info(f"å¹³å‡åˆ†å·®: {stats.avg_score_difference:.2f}")
    logger.info("=" * 70)
    
    # è´¨é‡æ£€æŸ¥
    quality_control = config.get('quality_control', {})
    min_valid_pairs = quality_control.get('min_valid_pairs', 10)
    
    if stats.valid_pairs < min_valid_pairs:
        logger.warning(f"âš ï¸ è­¦å‘Š: æœ‰æ•ˆæ ·æœ¬æ•° ({stats.valid_pairs}) å°‘äºæœ€å°è¦æ±‚ ({min_valid_pairs})")
        logger.warning("å»ºè®®è°ƒæ•´é€‰æ‹©ç­–ç•¥æˆ–å¢åŠ è¾“å…¥æ ·æœ¬æ•°")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ„é€ DPOè®­ç»ƒæ•°æ®")
    parser.add_argument(
        "--config",
        type=str,
        default="config/dpo_construction_config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = args.config
    if not os.path.isabs(config_path):
        config_path = os.path.join(project_root, config_path)
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(config_path):
        logger.error(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    # è¿è¡Œæ„é€ 
    try:
        construct_dpo_data(config_path)
    except Exception as e:
        logger.error(f"\nâŒ æ„é€ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
# python src/training/scripts/run_dpo_construction.py --config config/dpo_construction_config.yaml
