# DPOæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°æµç¨‹è¯¦è§£

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### DPOæ¨¡å‹çš„ç»„æˆ

```
DPOæ¨¡å‹ = åŸºç¡€æ¨¡å‹ + SFTçŸ¥è¯† + DPOå¯¹é½
```

ç”±äºä½¿ç”¨LoRAè®­ç»ƒï¼Œå®é™…ä¸Šæ¶‰åŠå¤šæ¬¡åˆå¹¶ï¼š

```
è®­ç»ƒé˜¶æ®µ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åŸºç¡€æ¨¡å‹    â”‚  Qwen2.5-1.5B-Instruct
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ + (merge)
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SFT LoRA    â”‚  model_output/qwen2_5_1_5b_instruct_sft/
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ = (merged model)
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚åŸºç¡€+SFTæ¨¡å‹ â”‚  [å†…å­˜ä¸­ï¼Œæœªä¿å­˜]
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ + (apply new LoRA)
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DPO LoRA    â”‚  [è®­ç»ƒæ—¶åˆ›å»º]
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ = (è®­ç»ƒå®Œæˆ)
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DPOæ¨¡å‹     â”‚  [å†…å­˜ä¸­]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ è®­ç»ƒæµç¨‹ï¼ˆrun_dpo_training.pyï¼‰

### æ­¥éª¤è¯¦è§£

#### 1. åŠ è½½SFTæ¨¡å‹

```python
# ç¬¬199-203è¡Œ
model, tokenizer = load_and_merge_lora_model(
    base_model_path=base_model_path,           # Qwen2.5-1.5B-Instruct
    lora_checkpoint_path=sft_checkpoint,        # SFT LoRA
    merge_lora=True  # âš ï¸ åˆå¹¶ï¼å¾—åˆ°å®Œæ•´æ¨¡å‹
)
```

**ç»“æœ**: `model` = åŸºç¡€æ¨¡å‹ + SFTçŸ¥è¯†ï¼ˆå®Œæ•´æ¨¡å‹ï¼ŒéLoRAï¼‰

#### 2. é…ç½®DPO LoRA

```python
# ç¬¬311è¡Œ
peft_config = setup_lora_config(lora_config)  # æ–°çš„DPO LoRAé…ç½®
```

**æ³¨æ„**: è¿™æ˜¯ä¸€ä¸ª**æ–°çš„**LoRAé…ç½®ï¼Œç”¨äºDPOè®­ç»ƒ

#### 3. DPOè®­ç»ƒ

```python
# ç¬¬407è¡Œ
trainer = DPOTrainer(
    model=model,              # å®Œæ•´æ¨¡å‹ï¼ˆåŸºç¡€+SFTï¼‰
    peft_config=peft_config,  # æ–°çš„DPO LoRA
    ...
)

# ç¬¬424è¡Œ
train_result = trainer.train()  # è®­ç»ƒDPO LoRA
```

**DPOTrainerä¼šåšä»€ä¹ˆ**:
- å°† `peft_config` åº”ç”¨åˆ° `model` ä¸Š
- è®­ç»ƒè¿™ä¸ªæ–°çš„DPO LoRAé€‚é…å™¨
- é€‚é…å™¨å­¦ä¹ çš„æ˜¯"å¦‚ä½•åœ¨SFTåŸºç¡€ä¸Šè¿›ä¸€æ­¥å¯¹é½"

#### 4. ä¿å­˜æ¨¡å‹ï¼ˆä¿®æ”¹åï¼‰

```python
# ç¬¬427-454è¡Œï¼ˆæ–°ç‰ˆæœ¬ï¼‰
if save_merged and isinstance(model, PeftModel):
    # åˆå¹¶DPO LoRA
    merged_model = model.merge_and_unload()
    merged_model.save_pretrained(output_dir)
    
    # ä¿å­˜çš„æ˜¯å®Œæ•´æ¨¡å‹ï¼šåŸºç¡€ + SFT + DPO
```

**ä¿å­˜å†…å®¹**:
- `model_output/qwen2_5_1_5b_dpo/`: å®Œæ•´åˆå¹¶æ¨¡å‹
- `model_output/qwen2_5_1_5b_dpo/dpo_lora_adapter/`: DPO LoRAé€‚é…å™¨ï¼ˆå¯é€‰ï¼‰

## ğŸ“Š è¯„ä¼°æµç¨‹

### é…ç½®æ–‡ä»¶ (dpo_evaluation_config.yaml)

```yaml
model:
  model_path: "model_output/qwen2_5_1_5b_dpo"  # å®Œæ•´æ¨¡å‹
  is_lora: false  # âœ… ä¸æ˜¯LoRAï¼Œæ˜¯å®Œæ•´æ¨¡å‹
  merge_lora: false  # âœ… ä¸éœ€è¦åˆå¹¶
```

### è¿è¡Œè¯„ä¼°

```bash
python src/training/scripts/run_evaluation.py \
    --config_path config/dpo_evaluation_config.yaml
```

### è¯„ä¼°æŒ‡æ ‡

1. **é€šè¿‡ç‡** - DPOåº”è¯¥æé«˜
2. **è¶Šæƒç‡** - DPOåº”è¯¥é™ä½ï¼ˆæ›´å®‰å…¨ï¼‰
3. **å¹»è§‰ç‡** - DPOåº”è¯¥é™ä½ï¼ˆæ›´å‡†ç¡®ï¼‰
4. **çº¢æ——é—æ¼ç‡** - ä¿æŒä½æ°´å¹³

## ğŸ†š ä¸‰ç§æ¨¡å‹å¯¹æ¯”

### 1. åŸºç¡€æ¨¡å‹
```yaml
model_path: "Qwen2.5-1.5B-Instruct/qwen/Qwen2___5-1___5B-Instruct"
is_lora: false
```

### 2. SFTæ¨¡å‹
```yaml
model_path: "model_output/qwen2_5_1_5b_instruct_sft"  # LoRA
base_model_path: "Qwen2.5-1.5B-Instruct/..."
is_lora: true
merge_lora: true  # éœ€è¦åˆå¹¶æ‰èƒ½ä½¿ç”¨
```

### 3. DPOæ¨¡å‹ï¼ˆæ–°ç‰ˆï¼‰
```yaml
model_path: "model_output/qwen2_5_1_5b_dpo"  # å®Œæ•´æ¨¡å‹
is_lora: false  # å·²ç»æ˜¯å®Œæ•´æ¨¡å‹
merge_lora: false
```

## âš ï¸ å¸¸è§é”™è¯¯

### é”™è¯¯1: DPOè¯„ä¼°ä½¿ç”¨åŸå§‹åŸºç¡€æ¨¡å‹

```yaml
# âŒ é”™è¯¯é…ç½®
model:
  model_path: "model_output/qwen2_5_1_5b_dpo"  # DPO LoRA
  base_model_path: "Qwen2.5-1.5B-Instruct"     # åŸå§‹åŸºç¡€æ¨¡å‹
  is_lora: true
```

**é—®é¢˜**: DPO LoRAæ˜¯åŸºäº"åŸºç¡€+SFT"è®­ç»ƒçš„ï¼Œä¸èƒ½ç›´æ¥åŠ è½½åˆ°åŸå§‹åŸºç¡€æ¨¡å‹

**è§£å†³**: ä¿å­˜å®Œæ•´åˆå¹¶æ¨¡å‹

### é”™è¯¯2: åªä¿å­˜LoRAé€‚é…å™¨

```python
# âŒ æ—§ç‰ˆæœ¬
trainer.save_model(output_dir)  # åªä¿å­˜DPO LoRA
```

**é—®é¢˜**: 
- ä¿å­˜çš„DPO LoRAéœ€è¦"åŸºç¡€+SFT"æ‰èƒ½ä½¿ç”¨
- ä½†"åŸºç¡€+SFT"æ²¡æœ‰ä¿å­˜

**è§£å†³**: åˆå¹¶åä¿å­˜å®Œæ•´æ¨¡å‹

## ğŸ“ ç›®å½•ç»“æ„

```
model_output/
â”œâ”€â”€ qwen2_5_1_5b_instruct_sft/        # SFT LoRAé€‚é…å™¨
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â””â”€â”€ adapter_model.safetensors
â”‚
â””â”€â”€ qwen2_5_1_5b_dpo/                 # DPOå®Œæ•´æ¨¡å‹ï¼ˆæ–°ç‰ˆï¼‰
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model.safetensors             # å®Œæ•´æ¨¡å‹æƒé‡
    â”œâ”€â”€ tokenizer*
    â””â”€â”€ dpo_lora_adapter/             # å¯é€‰ï¼šDPO LoRA
        â”œâ”€â”€ adapter_config.json
        â””â”€â”€ adapter_model.safetensors
```

## ğŸ¯ æœ€ä½³å®è·µ

### è®­ç»ƒé…ç½®

```yaml
# config/dpo_training_config.yaml
model:
  base_model_path: "Qwen2.5-1.5B-Instruct/..."
  sft_checkpoint_path: "model_output/qwen2_5_1_5b_instruct_sft"
  is_lora: true
  save_merged_dpo: true  # âœ… ä¿å­˜å®Œæ•´æ¨¡å‹

lora:
  enabled: true  # ä½¿ç”¨LoRAè®­ç»ƒDPOï¼ˆçœæ˜¾å­˜ï¼‰
```

### è¯„ä¼°é…ç½®

```yaml
# config/dpo_evaluation_config.yaml
model:
  model_path: "model_output/qwen2_5_1_5b_dpo"
  is_lora: false  # å®Œæ•´æ¨¡å‹
  
baseline_comparison:
  enabled: true
  baseline_model_path: "model_output/qwen2_5_1_5b_instruct_sft"
  baseline_is_lora: true  # SFTæ˜¯LoRA
```

### å¯¹æ¯”è¯„ä¼°è„šæœ¬

```bash
# 1. è¯„ä¼°SFTæ¨¡å‹ï¼ˆåŸºçº¿ï¼‰
python src/training/scripts/run_evaluation.py \
    --config_path config/evaluation_config.yaml

# 2. è¯„ä¼°DPOæ¨¡å‹
python src/training/scripts/run_evaluation.py \
    --config_path config/dpo_evaluation_config.yaml

# 3. å¯¹æ¯”ç»“æœ
python scripts/compare_models.py \
    output/evaluation/ \
    output/evaluation_dpo/
```

## ğŸ“ˆ æœŸæœ›çš„æ”¹è¿›

DPOè®­ç»ƒåï¼Œé¢„æœŸçœ‹åˆ°ï¼š

| æŒ‡æ ‡ | SFT | DPO | æ”¹è¿› |
|------|-----|-----|------|
| é€šè¿‡ç‡ | 45% | **55%+** | â¬†ï¸ +10% |
| è¶Šæƒç‡ | 25% | **15%** | â¬‡ï¸ -10% |
| å¹»è§‰ç‡ | 20% | **15%** | â¬‡ï¸ -5% |
| çº¢æ——é—æ¼ | 10% | **5%** | â¬‡ï¸ -5% |

**å…³é”®æ”¹è¿›ç‚¹**:
- âœ… æ›´å®‰å…¨ï¼šå‡å°‘ç¡®è¯Šã€ç»™å‰‚é‡ç­‰è¶Šæƒè¡Œä¸º
- âœ… æ›´å‡†ç¡®ï¼šå‡å°‘ç¼–é€ ä¿¡æ¯å’Œé”™è¯¯
- âœ… æ›´åˆè§„ï¼šä¿æŒæˆ–æå‡æ€¥ç—‡è¯†åˆ«èƒ½åŠ›

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜: è¯„ä¼°æ—¶åŠ è½½å¤±è´¥

```
Error: Unable to load adapter
```

**æ£€æŸ¥**:
```bash
# 1. ç¡®è®¤DPOæ¨¡å‹æ˜¯å¦æ˜¯å®Œæ•´æ¨¡å‹
ls -lh model_output/qwen2_5_1_5b_dpo/*.safetensors

# 2. æ£€æŸ¥æ˜¯å¦æœ‰adapter_config.jsonï¼ˆå¦‚æœæœ‰ï¼Œè¯´æ˜æ˜¯LoRAï¼‰
ls model_output/qwen2_5_1_5b_dpo/adapter_config.json

# 3. ç¡®è®¤è¯„ä¼°é…ç½®
cat config/dpo_evaluation_config.yaml | grep "is_lora"
```

**ä¿®å¤**:
- å¦‚æœä¿å­˜çš„æ˜¯LoRA: é‡æ–°è®­ç»ƒå¹¶ä¿å­˜å®Œæ•´æ¨¡å‹
- å¦‚æœä¿å­˜çš„æ˜¯å®Œæ•´æ¨¡å‹: è®¾ç½® `is_lora: false`

---

**æ›´æ–°æ—¶é—´**: 2026-02-01  
**ç‰ˆæœ¬**: v2.0ï¼ˆä¿®å¤DPOæ¨¡å‹ä¿å­˜é€»è¾‘ï¼‰
